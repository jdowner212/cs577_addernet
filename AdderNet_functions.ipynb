{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e0fhNZSGfHwc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root = os.getcwd() # whatever you want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q_XpZyGb_QSq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-14 10:47:53.294130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-11-14 10:47:53.384419: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-11-14 10:47:53.758614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quinlan/anaconda3/envs/tf/lib/\n",
            "2022-11-14 10:47:53.758661: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/quinlan/anaconda3/envs/tf/lib/\n",
            "2022-11-14 10:47:53.758665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# import torch\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GcKuPTjCjLEo"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Keeping track of the equations described in the paper -- not explicitly called\n",
        "in subsequent code but want to make sure the functionality is present\n",
        "'''\n",
        "\n",
        "####################################\n",
        "# defined in paper but not needed: #\n",
        "####################################\n",
        "\n",
        "'''Equation 1'''\n",
        "# ignore -- updated with Equation 2\n",
        "\n",
        "'''Equation 3'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 4'''\n",
        "# ignore -- updated with Equation 5\n",
        "\n",
        "'''Equation 8'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "\n",
        "####################################\n",
        "# explicitly used in code -- keep: #\n",
        "####################################\n",
        "def L1(a,b):\n",
        "    return -1*np.abs(a-b)\n",
        "\n",
        "'''Equation 7'''\n",
        "def hard_tanh(array):\n",
        "    array = np.where(array<-1,-1,array)\n",
        "    array = np.where(array>1, 1, array)\n",
        "    return array\n",
        "\n",
        "######################################################\n",
        "# implicitly used in code -- can technically delete: #\n",
        "######################################################\n",
        "\n",
        "'''Equation 2'''\n",
        "def Y_adder(image, F, m, n, t, similarity_f=L1): # image, group fo filters, row#, col#, filter#, similarity function\n",
        "    sum_ = 0\n",
        "    num_filters, k_depth, k_height, k_width = F.shape\n",
        "    for k in range(k_depth):\n",
        "        for j in range(k_width):\n",
        "            for i in range(k_height):\n",
        "                sum_ += similarity_f(image[k, m+i, n+j], F[t,k,i,j])\n",
        "    return sum_\n",
        "\n",
        "'''Equation 5'''\n",
        "def dY_dF_element(image,filters,m,n,i,j,k,t):\n",
        "    return image[k,m+i,n+j] - filter[t,k,i,j]\n",
        "\n",
        "'''Equation 6''' # clipped, full-precision gradient\n",
        "def dY_dImage_element(image,filters,m,n,i,j,k,t):\n",
        "    return hard_tanh(filter[t,k,i,j] - image[k,m+i,n+j])\n",
        "\n",
        "\n",
        "###################################################\n",
        "# written last week, haven't gotten to these yet: #\n",
        "###################################################\n",
        "\n",
        "'''Equation 9'''\n",
        "def var_Y_adder(X,F,variance_f=K.var):\n",
        "    # Not sure K.var is the function we want here, if we need to specify axis, etc.\n",
        "    var_X = variance_f(X)\n",
        "    var_F = variance_f(F)\n",
        "    ###\n",
        "    _, c_in, d, _ = F.shape\n",
        "    pi = np.pi\n",
        "    return np.sqrt(pi/2)*(d**2)*(c_in)*(var_X + var_F)\n",
        "\n",
        "'''Equation 10'''\n",
        "def batch_norm(minibatch, gamma, beta):\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    gamma*(minibatch-mean)/std + beta\n",
        "    return gamma*(minibatch-mean)/std + beta\n",
        "\n",
        "'''Equation 11'''\n",
        "def dL_dMinibatch_i(minibatch,dL_dy,i,L,gamma): # confused by this notation and need to revisit paper\n",
        "    # In dL_dy, y is the result of applying batch_norm to the minibatch\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    \n",
        "    sum_ = 0\n",
        "    for j in range(m):\n",
        "        x_term = (minibatch[i]-minibatch[j])*(minibatch[j]-mean)/std\n",
        "        sum_ += (dL_dy[i] - dL_dy[j]*(1 + x_term))\n",
        "    sum_ *= gamma/((m**2)*std)\n",
        "    \n",
        "    return sum_\n",
        "\n",
        "'''Equation 12'''\n",
        "# update rule for F\n",
        "def delta_F_l(adaptive_lr_l, dL_dF_l, gamma):\n",
        "    # the update delta for the filter in layer l\n",
        "    return gamma*adaptive_lr_l*dL_dF_l\n",
        "\n",
        "'''Equation 13'''\n",
        "def adaptive_lr_l(dL_dF_l, eta, k):\n",
        "    # k = number of elements in F_l -- I think equal to len(dL_dF_1)\n",
        "    # in which case we don't need to explicitly provide it\n",
        "    \n",
        "    # l2_norm = torch.sqrt([g**2 for g in dL_dF_l])\n",
        "    l2_norm = K.sqrt([g**2 for g in dL_dF_l]) # make sure torch.sqrt and K.sqrt are equivalents\n",
        "    \n",
        "    return eta*np.sqrt(k)/l2_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISUhuC_DB9_7"
      },
      "source": [
        "# Layer definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD9vhW-tCBj7"
      },
      "source": [
        "### `Layer` parent class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uH98VMolxy1J"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWAo_-8WCFZx"
      },
      "source": [
        "### `Adder` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CnPeNX0zDoEQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "dimension format/convention: NxHxWxC -- this is really unintuitive to me but i saw it in some tf \n",
        "documentation and thought it might make our lives easier later. Def not married to it\n",
        "'''\n",
        "\n",
        "\n",
        "def adder_single_step(window, filter, b=None, similarity_f=L1):\n",
        "    \"\"\"\n",
        "    window -- k_h x k_w x k_d\n",
        "    filter -- k_h x k_w x k_d\n",
        "    b      -- 1x1x1\n",
        "    Z      -- scalar\n",
        "    \"\"\"\n",
        "    H_k,H_w,D_k = filter.shape\n",
        "    out=0\n",
        "    for h in range(H_k):\n",
        "        for w in range(W_k):\n",
        "            for d in range(D_k):\n",
        "                out += similarity_f(window[h,w,d], filter[h,w,d])\n",
        "\n",
        "    if not b:\n",
        "        b = np.zeros((1,1,1))\n",
        "    out += b.astype(float)\n",
        "\n",
        "    return out\n",
        "\n",
        "class adder_layer(Layer):\n",
        "    def __init__(self,input_channels,output_channels,kernel_size=3,stride=1,padding=0,bias=None):\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.k=kernel_size\n",
        "        self.filters = np.random.normal(loc=0,scale=1,size=(self.output_channels, self.kernel_size, self.kernel_size, self.input_channsls))\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        bias = np.zeros((self.output_channels,1,1,1)) if not bias else bias\n",
        "        self.bias = bias\n",
        "\n",
        "    def get_adaptive_lr(self, k, dfilters, eta):\n",
        "        \"\"\"    \n",
        "        k           -- n_tensors \n",
        "        dfilters    -- c_out x k_H x k_W x c_in\n",
        "        eta         -- scalar\n",
        "        \"\"\"\n",
        "        norm = np.linalg.norm(dfilters, ord=2, axis=0)\n",
        "\n",
        "        return (eta * np.sqrt(k)) / norm\n",
        "\n",
        "    def forward(self,X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        filters -- c_out x k_H x k_W x c_in\n",
        "        b       -- c_out x 1 x 1 x 1\n",
        "        Z       -- n_tensors x H_new x W_new, c_out\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        self.input = X\n",
        "        filters,stride,padding,bias = self.filters, self.stride, self.padding, self.bias\n",
        "        n_tensors, H,   W,   c_in = X.shape\n",
        "        c_out,     H_k, W_k, c_in = filters.shape\n",
        "        n_filters = c_out\n",
        "\n",
        "        X_padded = np.pad(X, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        H_new = int((H + 2*padding - H_k)/stride)+1\n",
        "        W_new = int((W + 2*padding - W_k)/stride)+1\n",
        "\n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "\n",
        "        for i in range(n_tensors):           # traverse batch\n",
        "            this_img = X_padded[i,:,:,:]     # select ith image in batch\n",
        "            for f in range(n_filters):       # traverse filters\n",
        "                this_filter = filters[f,:,:,:]\n",
        "                this_bias = bias[f,:,:,:]\n",
        "                for h in range(H_new):       # traverse height\n",
        "                    for w in range(W_new):   # traverse width\n",
        "                        \n",
        "                        v0,v1 = h*stride, h*stride + H_k\n",
        "                        h0,h1 = w*stride, w*stride + W_k\n",
        "                        \n",
        "                        this_window = this_img[v0:v1,h0:h1,:]\n",
        "\n",
        "                        Z[i, h, w, f] = adder_single_step(this_window, this_filter, this_bias) \n",
        "\n",
        "        assert Z.shape == (n_tensors, H_new, W_new, n_filters)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, filters, bias, stride, padding\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate, adaptive_eta):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "        \n",
        "        X, filters, bias, stride, padding = self.cache\n",
        "\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        n_filters, H_k,    W_k,    c_down = filters.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "        \n",
        "        dX       = np.zeros_like(X)                        \n",
        "        dfilters = np.zeros_like(filters)\n",
        "        dbias    = np.zeros((n_filters, 1,1,1))\n",
        "\n",
        "        X_padded  = np.pad(X,  ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_padded = np.pad(dX, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_padded[i]\n",
        "            \n",
        "            for h in range(H_up):                   # traverse height\n",
        "                for w in range(W_up):               # traverse width\n",
        "                    for c in range(c_up):           # traverse filters\n",
        "                        \n",
        "                        v0,v1 = h,h+H_k\n",
        "                        h0,h1 = w,w+W_k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:h1, :]\n",
        "                        f_window = filters[c,:,:,:]\n",
        "\n",
        "                        dx_local = x_window-f_window\n",
        "                        df_local = hard_tanh(f_window-x_window)\n",
        "\n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += dx_local * g\n",
        "                        dfilters[c,:,:,:]   += df_local * g\n",
        "                        dbias[c,:,:,:]      += g\n",
        "                        \n",
        "            dX[i, :, :, :] = dx[padding:-padding, padding:-padding, :]\n",
        "        \n",
        "        assert(dX.shape == (n_tensors, H_down, W_down, c_down))\n",
        "\n",
        "        adaptive_lr = self.get_adaptive_lr(n_filters, dfilters, adaptive_eta)\n",
        "\n",
        "        self.filters -= learning_rate*adaptive_lr*dfilters\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### `BatchNorm` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class batch_norm_layer(Layer):\n",
        "    def __init__(self, input_channels, gamma=None, beta=None):\n",
        "        self.input_channels = input_channels\n",
        "        gamma = np.zeros((self.input_channels,1,1,1)) if not gamma else gamma\n",
        "        self.gamma = gamma\n",
        "        beta = np.ones((self.input_channels,1,1,1)) if not beta else beta\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        gamma   -- n_tensors x 1 x 1 x 1\n",
        "        beta    -- n_tensors x 1 x 1 x 1\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "\n",
        "        self.input = X\n",
        "        gamma, beta = self.gamma, self.beta\n",
        "        \n",
        "        mean = np.mean(X, axis=0)\n",
        "        std = np.std(X, axis=0)\n",
        "        X_center = X - mean\n",
        "        X_norm = X_center/std\n",
        "\n",
        "        self.output = gamma*X_norm + beta\n",
        "        \n",
        "        self.cache = X, X_center, X_norm\n",
        "\n",
        "        return self.output \n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, X_norm)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "\n",
        "        X, X_norm = self.cache\n",
        "\n",
        "        dGamma = np.sum(upstream_g * X_norm, axis=0)\n",
        "        dBeta = np.sum(upstream_g, axis=0)\n",
        "\n",
        "        m = len(X)\n",
        "        mean = np.mean(X)\n",
        "        std = np.std(X)\n",
        "        \n",
        "        dX = np.zeros_like(X)\n",
        "\n",
        "        for i in range(m):\n",
        "            for j in range(m):\n",
        "                dX[i] += (upstream_g[i] - upstream_g[j]*(1 + (X[i]-X[j])*(X[j]-mean)/std))\n",
        "            dX[i] *= self.gamma/((m**2)*std)\n",
        "        \n",
        "        self.gamma -= learning_rate*dGamma\n",
        "        self.beta -= learning_rate*dBeta\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO7MWk0_B3XO"
      },
      "source": [
        "### Activation layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jLa7JtufB6Tr"
      },
      "outputs": [],
      "source": [
        "def relu_fwd(X):\n",
        "    return np.where(X>=0,X,0)\n",
        "def relu_bwd(X):\n",
        "    return np.where(X>=0,1,0)\n",
        "def softmax_fwd(X):\n",
        "\texp_ = np.exp(X)\n",
        "\treturn exp_ / np.sum(exp_)\n",
        "def softmax_bwd(X):\n",
        "    I = np.eye(X.shape[0])\n",
        "    s = softmax_fwd(X)\n",
        "    return s*(I - s.T)\n",
        "def sig_fwd(X):\n",
        "    return 1/(1 + np.exp(-X))\n",
        "def sig_bwd(X):\n",
        "    return sig_fwd(X) * (1 - sig_fwd(X))\n",
        "\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self,act_forward,act_backward):\n",
        "        super(Layer, self).__init__()\n",
        "        self.fwd=act_forward\n",
        "        self.bwd=act_backward\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = self.fwd(X)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        local_g = self.bwd(self.input)\n",
        "        return local_g*upstream_g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKx4NGICC-74"
      },
      "source": [
        "### `Model` class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iaEag2IkC-JR"
      },
      "outputs": [],
      "source": [
        "loss_dict = {'sigmoid': {'forward':  None,\n",
        "                         'backward': None},\n",
        "             'softmax': {'forward':  None,\n",
        "                         'backward': None}}\n",
        "\n",
        "def single_example_accuracy(y_real,y_pred):\n",
        "    if y_pred.shape[-1] == 1: # i.e. if not one-hot encoded\n",
        "        if np.max(y_pred)>1:  # multi-category integer labels\n",
        "            pass\n",
        "        else:\n",
        "            y_real=1 if y_real.astype(float)>=0.5 else 0\n",
        "            y_pred=1 if y_pred.astype(float)>=0.5 else 0\n",
        "    else:\n",
        "        y_real = np.argmax(y_real)\n",
        "        y_pred = np.argmax(y_pred)\n",
        "    \n",
        "    return 1 if y_real == y_pred else 0\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,loss_name): # planning on creating a dictionary so we can get a loss function (forward + backward) from its name\n",
        "        self.layers = []\n",
        "        self.loss_fwd = loss_dict[loss_name]['forward']\n",
        "        self.loss_bwd = loss_dict[loss_name]['backward']\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        y_hat = []\n",
        "\n",
        "        for i in range(input_data.shape[0]):\n",
        "            Z = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                Z = layer.forward(Z)\n",
        "            y_hat.append(Z)\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate, x_val=None, y_val=None):\n",
        "\n",
        "        history = {'accuracy': [],\n",
        "                   'loss': [],\n",
        "                   'val_accuracy': [],\n",
        "                   'val_loss': []}\n",
        "\n",
        "        for e in range(epochs):\n",
        "            loss = 0\n",
        "            acc  = 0\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            for j in range(x_train.shape[0]):\n",
        "\n",
        "                # forward\n",
        "                Z = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    Z = layer.forward(Z)\n",
        "\n",
        "                y_real = y_train[j]\n",
        "                y_pred = Z\n",
        "\n",
        "                # compute loss and accuracy\n",
        "                loss += self.loss_fwd(y_real, y_pred)\n",
        "                acc  += single_example_accuracy(y_real, y_pred)\n",
        "\n",
        "                # backwward\n",
        "                error = self.loss_bwd(y_real, y_pred)\n",
        "                for layer in (self.layers)[::-1]:\n",
        "                    error = layer.backward(error, learning_rate)\n",
        "\n",
        "\n",
        "            loss /= x_train.shape[0]\n",
        "            acc  /= x_train.shape[0]\n",
        "            history['accuracy'].append(acc)\n",
        "            history['loss'].append(loss)\n",
        "\n",
        "            if not x_val or not y_val:\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss,3))}   acc = {str(round(acc,3))}')\n",
        "\n",
        "            elif x_val and y_val:\n",
        "                for k in range(x_val.shape[0]):\n",
        "                    Z_val = x_val[j]\n",
        "                    for layer in self.layers:\n",
        "                        Z_val = layer.forward(Z_val)\n",
        "\n",
        "                    y_real_val = y_val[k]\n",
        "                    y_pred_val = Z_val\n",
        "\n",
        "                    val_loss += self.loss_fwd(y_real_val, y_pred_val)\n",
        "                    val_acc  += single_example_accuracy(y_real_val, y_pred_val)\n",
        "\n",
        "                history['val_accuracy'].append(val_acc)\n",
        "                history['val_loss'].append(val_loss)\n",
        "\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss,3))}   acc = {str(round(acc,3))}   val_loss = {str(round(val_loss,3))}   val_accuracy = {str(round(val_acc,3))}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kRIJNY0dRvD2"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "__init__() missing 1 required positional argument: 'loss_name'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m this_model \u001b[39m=\u001b[39m Model()\n\u001b[1;32m      3\u001b[0m this_model\u001b[39m.\u001b[39madd(adder_layer)\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'loss_name'"
          ]
        }
      ],
      "source": [
        "this_model = Model()\n",
        "\n",
        "this_model.add(adder_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbHmgJRtCj69"
      },
      "source": [
        "## Data collection\n",
        "\n",
        "### p.s. - been having a hard time finding an imagenet dataset/subset that is a reasonable size and easy to download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oo5KOmoCzNm"
      },
      "source": [
        "### CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "uHl1tum3l-oL"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow.keras.utils as np_utils\n",
        "\n",
        "\n",
        "#!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "# data_zip = os.path.join(root,'cifar-10-python.tar.gz')\n",
        "# f = tarfile.open(data_zip)\n",
        "# f.extractall(root) \n",
        "# f.close()\n",
        "# os.remove(data_zip)\n",
        "\n",
        "def load_cifar_data(folder):\n",
        "    train_batches = [f'{folder}/{f}' for f in os.listdir(folder) if 'batch_' in f]\n",
        "    test_batch    =  f'{folder}/test_batch'\n",
        "\n",
        "    # Get train data\n",
        "    X_trn = None\n",
        "    y_trn = []\n",
        "    for i in range(len(train_batches)):\n",
        "        train_data_dict = pickle.load(open(train_batches[i],'rb'), encoding='latin-1')\n",
        "        if i+1 == 1:\n",
        "            X_trn = train_data_dict['data']\n",
        "        else:\n",
        "            X_trn = np.vstack((X_trn, train_data_dict['data']))\n",
        "        y_trn += train_data_dict['labels']\n",
        "    X_trn = X_trn.reshape(len(X_trn),3,32,32)\n",
        "    X_trn = np.rollaxis(X_trn,1,4)\n",
        "    X_trn = X_trn.astype('float32')/255.0\n",
        "    y_trn = np_utils.to_categorical(np.asarray(y_trn),10)\n",
        "\n",
        "    # Get test data\n",
        "    test_data_dict  = pickle.load(open(test_batch,'rb'), encoding='latin-1')\n",
        "    X_tst = test_data_dict['data']\n",
        "    X_tst = X_tst.reshape(len(X_tst),3,32,32)\n",
        "    X_tst = np.rollaxis(X_tst,1,4)\n",
        "    X_tst = X_tst.astype('float32')/255.0\n",
        "    y_tst = np_utils.to_categorical(np.asarray(test_data_dict['labels']))\n",
        "    \n",
        "    n_90 = int(0.9*len(X_trn))\n",
        "    X_trn, X_val = X_trn[:n_90], X_trn[n_90:]\n",
        "    y_trn, y_val = y_trn[:n_90], y_trn[n_90:]\n",
        "\n",
        "    return X_trn, y_trn, X_tst, y_tst, X_val, y_val\n",
        "\n",
        "task_2_data_dir = f'{root}/cifar-10-batches-py'\n",
        "X_trn_c10, y_trn_c10, X_tst_c10, y_tst_c10, X_val_c10, y_val_c10 = load_cifar_data(task_2_data_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO8HnUApFnjb1YecNzXcwlu",
      "collapsed_sections": [],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('tf')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "6ebc4698c5ff6fbb91288e5ca8c10cf5bf30ed073fdd7a9c07ef179455218630"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
