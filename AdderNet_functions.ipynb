{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPKkwtLosaeR66/BT4PPHvi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "Q_XpZyGb_QSq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4LmN5vhI-2d7"
      },
      "outputs": [],
      "source": [
        "from torch.torch_version import TorchVersion\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def L1(a,b):\n",
        "    return -1*np.abs(a-b)\n",
        "\n",
        "def hard_tanh(value):\n",
        "    if -1 < value and value < 1:\n",
        "        return value\n",
        "    elif value > 1:\n",
        "        return 1\n",
        "    elif value < -1:\n",
        "        return -1\n",
        "\n",
        "'''Equation 1'''\n",
        "# modified by Equation 2\n",
        "\n",
        "\n",
        "'''Equation 2'''\n",
        "def Y_adder(X, F, m, n, t, similarity_f=L1):\n",
        "    # F.shape returns (#filters, #channels, #rows, #columns)\n",
        "    # X.shape returns (#channels, #rows, #columns)\n",
        "    # t specifies filter #\n",
        "    sum_ = 0\n",
        "    _, c_in, d, _ = F.shape     \n",
        "    for k in range(c_in):\n",
        "        for j in range(d):\n",
        "            for i in range(d):\n",
        "                sum_ += similarity_f(X[k, m+i, n+j], F[i, j, k, t])\n",
        "    return sum_\n",
        "\n",
        "'''Equation 3'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 4'''\n",
        "# ignore -- updated with equation 5\n",
        "\n",
        "'''Equation 5'''\n",
        "def dY_dF(X,F,m,n,i,j,k,t):\n",
        "    X_ = X[k,m+i,n+j]\n",
        "    F_ = F[t,k,i,j]\n",
        "    return X_ - F_\n",
        "\n",
        "'''Equation 6'''\n",
        "def dY_dX(X,F,m,n,i,j,k,t):\n",
        "    # clipped, full-precision gradient\n",
        "    X_ = X[k,m+i,n+j]\n",
        "    F_ = F[t,k,i,j]\n",
        "    return hard_tanh(X_ - F_)\n",
        "\n",
        "'''Equation 7'''\n",
        "# ignore -- hard_tanh implemented previously\n",
        "\n",
        "'''Equation 8'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 9'''\n",
        "def var_Y_adder(X,F,variance_f=torch.var):\n",
        "    # check torch.var documentation: https://pytorch.org/docs/stable/generated/torch.var.html\n",
        "    # not sure if we can call torch.var(X) with default parameters\n",
        "    # or if we need to specify. Does this output a scalar or a tensor?\n",
        "    var_X = variance_f(X)\n",
        "    var_F = variance_f(F)\n",
        "    ###\n",
        "    _, c_in, d, _ = F.shape\n",
        "    pi = np.pi\n",
        "\n",
        "    return np.sqrt(pi/2)*(d**2)*(c_in)*(var_X + var_F)\n",
        "\n",
        "'''Equation 10'''\n",
        "def batch_norm(minibatch, gamma, beta):\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    gamma*(minibatch-mean)/std + beta\n",
        "    return gamma*(minibatch-mean)/std + beta\n",
        "\n",
        "'''Equation 11'''\n",
        "def dL_dMinibatch_i(minibatch,dL_dy,i,L,gamma):\n",
        "    # In dL_dy, y is the result of applying batch_norm to the minibatch\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    \n",
        "    sum_ = 0\n",
        "    for j in range(m):\n",
        "        x_term = (minibatch[i]-minibatch[j])*(minibatch[j]-mean)/std\n",
        "        sum_ += (dL_dy[i] - dL_dy[j]*(1 + x_term))\n",
        "    sum_ *= gamma/((m**2)*std)\n",
        "    \n",
        "    return sum_\n",
        "\n",
        "'''Equation 12'''\n",
        "def delta_F_l(adaptive_lr_l, dL_dF_l, gamma):\n",
        "    # the update delta for the filter in layer l\n",
        "    return gamma*adaptive_lr_l*dL_dF_l\n",
        "\n",
        "'''Equation 13'''\n",
        "def adaptive_lr_l(dL_dF_l, eta, k):\n",
        "    # k = number of elements in F_l -- I think equal to len(dL_dF_1)\n",
        "    # in which case we don't need to explicitly provide it\n",
        "    l2_norm = torch.sqrt([g**2 for g in dL_dF_l])\n",
        "    return eta*np.sqrt(k)/l2_norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bj43PLAfOk2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}