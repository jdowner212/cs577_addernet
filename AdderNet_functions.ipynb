{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwz3i4XqwMRkoYIhh/+1xv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "import skimage\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n",
        "\n",
        "# so we can run original addernet:\n",
        "from torch.torch_version import TorchVersion\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "Q_XpZyGb_QSq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFWTCTkGjW81",
        "outputId": "6d7c6a39-91ff-4169-ab09-d63b40f3f518"
      },
      "execution_count": 224,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 224
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "id": "4LmN5vhI-2d7"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Functions described in paper\n",
        "'''\n",
        "\n",
        "def L1(a,b):\n",
        "    return -1*np.abs(a-b)\n",
        "\n",
        "def hard_tanh(array):\n",
        "    array = np.where(array<-1,-1,array)\n",
        "    array = np.where(array>1, 1, array)\n",
        "    return array\n",
        "\n",
        "'''Equation 1'''\n",
        "# modified by Equation 2\n",
        "\n",
        "'''Equation 5'''\n",
        "def dY_dF_element(image,filters,m,n,i,j,k,t):\n",
        "    return image[k,m+i,n+j] - filter[t,k,i,j]\n",
        "\n",
        "'''Equation 6''' # clipped, full-precision gradient\n",
        "def dY_dImage_element(image,filters,m,n,i,j,k,t):\n",
        "    return hard_tanh(filter[t,k,i,j] - image[k,m+i,n+j])\n",
        "\n",
        "def window_gradients(image,filters,m,n,t):\n",
        "    dy_df_window = np.zeros_like(filters)\n",
        "    dy_dx_window = np.zeros_like(image)\n",
        "\n",
        "    num_filters, k_depth, k_height, k_width = filters.shape\n",
        "    for k in range(k_depth):\n",
        "        for j in range(k_width):\n",
        "            for i in range(k_height):\n",
        "                img_minus_f = dY_dF_element(image,filters,m,n,i,j,k,t)\n",
        "                dy_df_window[t,k,i,j] = img_minus_f\n",
        "                dy_dx_window[k,i,j]   = hard_tanh(img_minus_f)\n",
        "                \n",
        "    return dy_df_window, dy_dx_window\n",
        "\n",
        "'''Equation 2'''\n",
        "def Y_adder(image, F, m, n, t, similarity_f=L1): # image, group fo filters, row#, col#, filter#, similarity function\n",
        "    sum_ = 0\n",
        "    num_filters, k_depth, k_height, k_width = F.shape\n",
        "    for k in range(k_depth):\n",
        "        for j in range(k_width):\n",
        "            for i in range(k_height):\n",
        "                sum_ += similarity_f(image[k, m+i, n+j], F[t,k,i,j])\n",
        "    return sum_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "XXD1EBq5JSg8"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = np.random.randint(0,10,(1,3,5,5))\n",
        "filters = np.random.randint(0,10,(5,3,3,3))\n",
        "forward_batch(images,filters,1,0)#.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo8oVetNd7IC",
        "outputId": "3289f277-3ee0-4c20-d19d-e2f046db38e8"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[[ -85.,  -95.,  -71.],\n",
              "          [-107.,  -90., -103.],\n",
              "          [-109.,  -57., -101.]]],\n",
              "\n",
              "\n",
              "        [[[ -96.,  -82.,  -90.],\n",
              "          [ -88.,  -77.,  -82.],\n",
              "          [ -94.,  -94.,  -86.]]],\n",
              "\n",
              "\n",
              "        [[[ -92.,  -82.,  -96.],\n",
              "          [ -90.,  -85.,  -86.],\n",
              "          [-100.,  -82.,  -78.]]],\n",
              "\n",
              "\n",
              "        [[[-105.,  -83.,  -95.],\n",
              "          [ -93.,  -76., -111.],\n",
              "          [ -95.,  -97., -101.]]],\n",
              "\n",
              "\n",
              "        [[[ -94.,  -96., -100.],\n",
              "          [ -74.,  -87.,  -84.],\n",
              "          [ -88.,  -72.,  -84.]]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "O1osm2Hq8rse"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add.layers(tf.keras.layers.Conv2D(filters))"
      ],
      "metadata": {
        "id": "WzNqNWiX9MCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.layers.Conv2D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWjtjrOm91kH",
        "outputId": "f5644322-dcfd-46b6-b300-67d67404ae9a"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.layers.convolutional.conv2d.Conv2D"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''Equation 3'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 4'''\n",
        "# ignore -- updated with equation 5\n",
        "\n",
        "'''Equation 7'''\n",
        "# ignore -- hard_tanh implemented previously\n",
        "\n",
        "'''Equation 8'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 9'''\n",
        "# def var_Y_adder(X,F,variance_f=torch.var):\n",
        "    # check torch.var documentation: https://pytorch.org/docs/stable/generated/torch.var.html\n",
        "    # not sure if we can call torch.var(X) with default parameters\n",
        "    # or if we need to specify. Does this output a scalar or a tensor?\n",
        "# Trying K.var as tensorflow substitute for torch.var -- make sure they work the same\n",
        "# or tf.var?\n",
        "def var_Y_adder(X,F,variance_f=K.var):\n",
        "    var_X = variance_f(X)\n",
        "    var_F = variance_f(F)\n",
        "    ###\n",
        "    _, c_in, d, _ = F.shape\n",
        "    pi = np.pi\n",
        "\n",
        "    return np.sqrt(pi/2)*(d**2)*(c_in)*(var_X + var_F)\n",
        "\n",
        "'''Equation 10'''\n",
        "def batch_norm(minibatch, gamma, beta):\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    gamma*(minibatch-mean)/std + beta\n",
        "    return gamma*(minibatch-mean)/std + beta\n",
        "\n",
        "'''Equation 11'''\n",
        "def dL_dMinibatch_i(minibatch,dL_dy,i,L,gamma):\n",
        "    # In dL_dy, y is the result of applying batch_norm to the minibatch\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    \n",
        "    sum_ = 0\n",
        "    for j in range(m):\n",
        "        x_term = (minibatch[i]-minibatch[j])*(minibatch[j]-mean)/std\n",
        "        sum_ += (dL_dy[i] - dL_dy[j]*(1 + x_term))\n",
        "    sum_ *= gamma/((m**2)*std)\n",
        "    \n",
        "    return sum_\n",
        "\n",
        "'''Equation 12'''\n",
        "# update rule for F\n",
        "def delta_F_l(adaptive_lr_l, dL_dF_l, gamma):\n",
        "    # the update delta for the filter in layer l\n",
        "    return gamma*adaptive_lr_l*dL_dF_l\n",
        "\n",
        "'''Equation 13'''\n",
        "def adaptive_lr_l(dL_dF_l, eta, k):\n",
        "    # k = number of elements in F_l -- I think equal to len(dL_dF_1)\n",
        "    # in which case we don't need to explicitly provide it\n",
        "    \n",
        "    \n",
        "    # l2_norm = torch.sqrt([g**2 for g in dL_dF_l])\n",
        "    l2_norm = K.sqrt([g**2 for g in dL_dF_l]) # make sure torch.sqrt and K.sqrt are equivalents\n",
        "    \n",
        "    \n",
        "    return eta*np.sqrt(k)/l2_norm"
      ],
      "metadata": {
        "id": "GcKuPTjCjLEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward(X,F,Y_hat,Y,L):\n",
        "    print('\\n\\n**Backward**\\n\\n')\n",
        "    X     = self.X\n",
        "    Y     = self.Y\n",
        "    Y_hat = self.Y_hat\n",
        "    W_L1  = self.W_L1\n",
        "    W_L2  = self.W_L2\n",
        "    W_L3  = self.W_L3\n",
        "\n",
        "    # d_loss\n",
        "    d_yhat = self.loss.backward()\n",
        "    print('d_yhat.shape:       ', d_yhat.shape)\n",
        "\n",
        "    # layer 3\n",
        "    d_ZW3_plus_b   = self.softmax.backward(d_yhat)\n",
        "    [d_ZW3, d_b3]  = self.add__L3.backward(d_ZW3_plus_b)\n",
        "    [d_Z2,  d_W3]  = self.mult_L3.backward(d_ZW3)\n",
        "    print('d_ZW3_plus_b.shape: ',self.d_Z3_plus_b.shape)\n",
        "    print('d_W3.shape:         ',self.d_W3.shape)\n",
        "    print('d_Z2.shape:         ',self.d_Z2.shape)\n",
        "\n",
        "    # layer 2\n",
        "    d_ZW2_plus_b   = self.sig__L2.backward(d_Z2)\n",
        "    [d_ZW2, d_b2]  = self.add__L2.backward(d_ZW2_plus_b)\n",
        "    [d_Z1,   d_W2] = self.mult_L2.backward(d_ZW2)\n",
        "    print('d_ZW2_plus_b.shape: ',self.d_ZW2_plus_b.shape)\n",
        "    print('d_W2.shape:         ',self.dW2.shape)\n",
        "    print('d_Z1.shape:         ',self.d_Z1.shape)\n",
        "\n",
        "    # layer 1\n",
        "    d_ZW1_plus_b  = self.sig__L1.backward(d_Z1)\n",
        "    [d_XW1, d_b1] = self.add__L1.backward(d_ZW1_plus_b)\n",
        "    [d_X,   d_W1] = self.mult_L1.backward(d_XW1)\n",
        "    print('d_ZW1_plus_b.shape: ',self.d_sig_Z1.shape)\n",
        "    print('d_W1.shape:         ',self.d_W1.shape)\n",
        "    print('d_X.shape:          ',self.d_X.shape)\n",
        "\n",
        "    return d_W1, d_b1, d_W2, d_b2, d_W3, d_b3"
      ],
      "metadata": {
        "id": "oP0sXYpbYZLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "\n",
        "def adder2d_function(X, W, stride=1, padding=0):\n",
        "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
        "    n_x, d_x, h_x, w_x = X.size()\n",
        "\n",
        "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
        "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
        "\n",
        "    h_out, w_out = int(h_out), int(w_out)\n",
        "    X_col = torch.nn.functional.unfold(X.view(1, -1, h_x, w_x), h_filter, dilation=1, padding=padding, stride=stride).view(n_x, -1, h_out*w_out)\n",
        "    X_col = X_col.permute(1,2,0).contiguous().view(X_col.size(1),-1)\n",
        "    W_col = W.view(n_filters, -1)\n",
        "    \n",
        "    out = adder.apply(W_col,X_col)\n",
        "    \n",
        "    out = out.view(n_filters, h_out, w_out, n_x)\n",
        "    out = out.permute(3, 0, 1, 2).contiguous()\n",
        "    \n",
        "    return out\n",
        "\n",
        "class adder(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, W_col, X_col):\n",
        "        ctx.save_for_backward(W_col,X_col)\n",
        "        display(X_col)\n",
        "        display(W_col)\n",
        "        output = -(W_col.unsqueeze(2)-X_col.unsqueeze(0)).abs().sum(1)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx,grad_output):\n",
        "        W_col,X_col = ctx.saved_tensors\n",
        "        grad_W_col = ((X_col.unsqueeze(0)-W_col.unsqueeze(2))*grad_output.unsqueeze(1)).sum(2)\n",
        "        grad_W_col = grad_W_col/grad_W_col.norm(p=2).clamp(min=1e-12)*math.sqrt(W_col.size(1)*W_col.size(0))/5\n",
        "        grad_X_col = (-(X_col.unsqueeze(0)-W_col.unsqueeze(2)).clamp(-1,1)*grad_output.unsqueeze(1)).sum(0)\n",
        "        \n",
        "        return grad_W_col, grad_X_col\n",
        "    \n",
        "class adder2d(nn.Module):\n",
        "\n",
        "    def __init__(self,input_channel,output_channel,kernel_size, stride=1, padding=0, bias = False):\n",
        "        super(adder2d, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.adder = torch.nn.Parameter(nn.init.normal_(torch.randn(output_channel,input_channel,kernel_size,kernel_size)))\n",
        "        self.bias = bias\n",
        "        if bias:\n",
        "            self.b = torch.nn.Parameter(nn.init.uniform_(torch.zeros(output_channel)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = adder2d_function(x,self.adder, self.stride, self.padding)\n",
        "        if self.bias:\n",
        "            output += self.b.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "T9TGNNg8yhjZ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(images>5,5,images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULkHZr7jEn32",
        "outputId": "efcd297b-558b-485b-f9fd-5622a3163bb4"
      },
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[5, 5, 5, 5, 5],\n",
              "         [4, 0, 1, 3, 0],\n",
              "         [5, 3, 1, 5, 4],\n",
              "         [2, 5, 5, 5, 2],\n",
              "         [5, 4, 1, 3, 4]],\n",
              "\n",
              "        [[5, 5, 4, 4, 5],\n",
              "         [5, 5, 5, 5, 2],\n",
              "         [0, 5, 0, 2, 5],\n",
              "         [5, 5, 1, 5, 5],\n",
              "         [1, 5, 5, 5, 5]],\n",
              "\n",
              "        [[5, 5, 1, 5, 3],\n",
              "         [5, 0, 1, 5, 5],\n",
              "         [1, 0, 5, 3, 1],\n",
              "         [1, 1, 5, 3, 0],\n",
              "         [4, 3, 2, 5, 5]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 319
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dimensions: NxHxWxC\n",
        "\n",
        "def relu(array):\n",
        "    return np.where(array>=0,array,0)\n",
        "\n",
        "def addernet_single_step(window, filter, b, similarity_f=L1):\n",
        "    \"\"\"\n",
        "    window -- k_h x k_w x k_d\n",
        "    filter -- k_h x k_w x k_d\n",
        "    b      -- 1x1x1\n",
        "    Z      -- scalar\n",
        "    \"\"\"\n",
        "    k_h,k_w,k_d = filter.shape\n",
        "    Z=0\n",
        "    for h in range(k_h):\n",
        "        for w in range(k_w):\n",
        "            for d in range(k_d):\n",
        "                out += similarity_f(window[h,w,d], filter[h,w,d])\n",
        "\n",
        "    Z += b.astype(float)\n",
        "\n",
        "    return Z\n",
        "\n",
        "class addernet_layer:\n",
        "    def __init__(self,F,X,stride=1,padding=0,activation=relu,B=None):\n",
        "        self.F = F\n",
        "        self.X = X\n",
        "        self.s = stride\n",
        "        self.p = padding\n",
        "        self.act = activation\n",
        "        B = np.zeros((W.shape[0],1,1,1)) if not B else B\n",
        "        self.B = B\n",
        "\n",
        "    def addernet_forward(self):\n",
        "        \"\"\"    \n",
        "        X -- n_tensors x H x W x c_in\n",
        "        F -- n_filters x k_H x k_W x c_out\n",
        "        b -- n_filters x 1 x 1 x 1\n",
        "        Z -- n_tensors x H_new x W_new, c_out\n",
        "        cache -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        X,F,s,p,act,B = self.X, self.F, self.s, self.p, self.act, self.B\n",
        "        n_tensors, H,   W,   c_in  = X.shape\n",
        "        n_filters, k_H, k_W, c_out = F.shape\n",
        "\n",
        "        H_new = int((H + 2*p - k_H)/s)+1\n",
        "        W_new = int((W + 2*p - k_W)/s)+1\n",
        "        \n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "        X_padded = np.pad(X, ((0,0), (p,p), (p,p), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):            # loop over batches               \n",
        "            this_x = X_padded[i,:,:,:]        # select ith image in batch\n",
        "            for h in range(H_new):            # traverse height\n",
        "                for w in range(W_new):        # traverse width\n",
        "                    for c in range(c_out):    # traverse depth/filters\n",
        "                        \n",
        "                        v0 = h*s\n",
        "                        v1 = h*s + k_H\n",
        "                        h0 = w*s \n",
        "                        h1 = w*s + k_W\n",
        "                        \n",
        "                        this_window = this_x[v0:v1,h0:h1,:]\n",
        "\n",
        "                        Z[i, h, w, c] = addernet_single_step(this_window, F[:, :, :, c], B[:,:,:,c])\n",
        "\n",
        "        Z = act(Z)                             \n",
        "        assert Z.shape == (n_tensors, H_new, W_new, c_out)\n",
        "        cache = (X, W, B, s, p)\n",
        "        return Z, cache\n",
        "\n",
        "    def addernet_backward(upstream_g, cache):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dW -- dL/dW, shape k x k x k x n_filters\n",
        "        dB -- dL/dB, shape 1 x 1 x 1 x n_filters\n",
        "        \"\"\"\n",
        "        \n",
        "        X, W, B, s, p = cache\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        k, k, k, n_filters = W.shape\n",
        "        \n",
        "        n_tensors, H_up, W_up, c_up = upstream_g.shape\n",
        "        \n",
        "\n",
        "        dX_down = np.zeros((n_tensors, H_down, W_down, c_down))                           \n",
        "        dW = np.zeros((k, k, k, n_filters))\n",
        "        dB = np.zeros((1, 1, 1, n_filters))\n",
        "\n",
        "        X_padded = np.pad(X, ((0,0), (p,p), (p,p), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_down_padded = np.pad(dX_down, ((0,0), (p,p), (p,p), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_down_padded[i]\n",
        "            \n",
        "            for h in range(H_up):                   # loop over vertical axis of the output volume\n",
        "                for w in range(W_up):               # loop over horizontal axis of the output volume\n",
        "                    for c in range(c_up):           # loop over the channels of the output volume\n",
        "                        \n",
        "                        v0,v1 = h,h+k\n",
        "                        h0,h1 = w,w+k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:v1, :]\n",
        "                        dx_local = x_window-W[:,:,:,c]\n",
        "                        dw_local = hard_tanh(W[:,:,:,c]-x_window)\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += dx_local * upstream_g[i, h, w, c]\n",
        "                        dW[:,:,:,c] += dw_local * upstream_g[i, h, w, c]\n",
        "                        dB[:,:,:,c] += upstream_g[i, h, w, c]\n",
        "                        \n",
        "            dX_down[i, :, :, :] = dx[p:-p, p:-p, :]\n",
        "        \n",
        "        assert(dX_down.shape == (n_tensors, H_down, W_down, c_down))\n",
        "        return dX_down, dW, dB"
      ],
      "metadata": {
        "id": "CnPeNX0zDoEQ"
      },
      "execution_count": 320,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SXZRf9nrZJU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}