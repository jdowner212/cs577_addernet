{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBN8XFP31g1gmMrjjWh6+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "root = os.getcwd() # whatever you want"
      ],
      "metadata": {
        "id": "e0fhNZSGfHwc"
      },
      "execution_count": 354,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "np_config.enable_numpy_behavior()\n"
      ],
      "metadata": {
        "id": "Q_XpZyGb_QSq"
      },
      "execution_count": 355,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Keeping track of the equations described in the paper -- not explicitly called\n",
        "in subsequent code but want to make sure the functionality is present\n",
        "'''\n",
        "\n",
        "####################################\n",
        "# defined in paper but not needed: #\n",
        "####################################\n",
        "\n",
        "'''Equation 1'''\n",
        "# ignore -- updated with Equation 2\n",
        "\n",
        "'''Equation 3'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "'''Equation 4'''\n",
        "# ignore -- updated with Equation 5\n",
        "\n",
        "'''Equation 8'''\n",
        "# ignore -- CNN formula\n",
        "\n",
        "\n",
        "####################################\n",
        "# explicitly used in code -- keep: #\n",
        "####################################\n",
        "def L1(a,b):\n",
        "    return np.abs(a-b)\n",
        "\n",
        "'''Equation 7'''\n",
        "def hard_tanh(array):\n",
        "    array = np.where(array<-1,-1,array)\n",
        "    array = np.where(array>1, 1, array)\n",
        "    return array\n",
        "\n",
        "######################################################\n",
        "# implicitly used in code -- can technically delete: #\n",
        "######################################################\n",
        "\n",
        "'''Equation 2'''\n",
        "def Y_adder(image, F, m, n, t, similarity_f=L1): # image, group fo filters, row#, col#, filter#, similarity function\n",
        "    sum_ = 0\n",
        "    num_filters, k_depth, k_height, k_width = F.shape\n",
        "    for k in range(k_depth):\n",
        "        for j in range(k_width):\n",
        "            for i in range(k_height):\n",
        "                sum_ += similarity_f(image[k, m+i, n+j], F[t,k,i,j])\n",
        "    return sum_\n",
        "\n",
        "'''Equation 5'''\n",
        "def dY_dF_element(image,filters,m,n,i,j,k,t):\n",
        "    return image[k,m+i,n+j] - filter[t,k,i,j]\n",
        "\n",
        "'''Equation 6''' # clipped, full-precision gradient\n",
        "def dY_dImage_element(image,filters,m,n,i,j,k,t):\n",
        "    return hard_tanh(filter[t,k,i,j] - image[k,m+i,n+j])\n",
        "\n",
        "\n",
        "###################################################\n",
        "# written last week, haven't gotten to these yet: #\n",
        "###################################################\n",
        "\n",
        "'''Equation 9'''\n",
        "def var_Y_adder(X,F,variance_f=K.var):\n",
        "    # Not sure K.var is the function we want here, if we need to specify axis, etc.\n",
        "    var_X = variance_f(X)\n",
        "    var_F = variance_f(F)\n",
        "    ###\n",
        "    _, c_in, d, _ = F.shape\n",
        "    pi = np.pi\n",
        "    return np.sqrt(pi/2)*(d**2)*(c_in)*(var_X + var_F)\n",
        "\n",
        "'''Equation 10'''\n",
        "def batch_norm(minibatch, gamma, beta):\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    gamma*(minibatch-mean)/std + beta\n",
        "    return gamma*(minibatch-mean)/std + beta\n",
        "\n",
        "'''Equation 11'''\n",
        "def dL_dMinibatch_i(minibatch,dL_dy,i,L,gamma): # confused by this notation and need to revisit paper\n",
        "    # In dL_dy, y is the result of applying batch_norm to the minibatch\n",
        "    m = len(minibatch)\n",
        "    mean = (1/m)*sum(minibatch)\n",
        "    std = (1/m)*sum([(x_i-mean)**2 for x_i in minibatch])\n",
        "    \n",
        "    sum_ = 0\n",
        "    for j in range(m):\n",
        "        x_term = (minibatch[i]-minibatch[j])*(minibatch[j]-mean)/std\n",
        "        sum_ += (dL_dy[i] - dL_dy[j]*(1 + x_term))\n",
        "    sum_ *= gamma/((m**2)*std)\n",
        "    \n",
        "    return sum_\n",
        "\n",
        "'''Equation 12'''\n",
        "# update rule for F\n",
        "def delta_F_l(adaptive_lr_l, dL_dF_l, gamma):\n",
        "    # the update delta for the filter in layer l\n",
        "    return gamma*adaptive_lr_l*dL_dF_l\n",
        "\n",
        "'''Equation 13'''\n",
        "def adaptive_lr_l(dL_dF_l, eta, k):\n",
        "    # k = number of elements in F_l -- I think equal to len(dL_dF_1)\n",
        "    # in which case we don't need to explicitly provide it\n",
        "    \n",
        "    # l2_norm = torch.sqrt([g**2 for g in dL_dF_l])\n",
        "    l2_norm = K.sqrt([g**2 for g in dL_dF_l]) # make sure torch.sqrt and K.sqrt are equivalents\n",
        "    \n",
        "    return eta*np.sqrt(k)/l2_norm"
      ],
      "metadata": {
        "id": "GcKuPTjCjLEo"
      },
      "execution_count": 356,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer definitions"
      ],
      "metadata": {
        "id": "ISUhuC_DB9_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2><strong><font color='red'>still need:<br><h4><font color='red'>- fully-connected<br>- batchnorm<br>- adaptive learning rate<br>- ?</font></h4></font></strong></h2>\n"
      ],
      "metadata": {
        "id": "Pa4TOj4eR9F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Layer` parent class"
      ],
      "metadata": {
        "id": "LD9vhW-tCBj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        raise NotImplementedError"
      ],
      "metadata": {
        "id": "uH98VMolxy1J"
      },
      "execution_count": 357,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Adder` layer"
      ],
      "metadata": {
        "id": "bWAo_-8WCFZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "dimension format/convention: NxHxWxC -- this is really unintuitive to me but i saw it in some tf \n",
        "documentation and thought it might make our lives easier later. Def not married to it\n",
        "'''\n",
        "\n",
        "\n",
        "def adder_single_step(window, filter, similarity_f=L1):\n",
        "    \"\"\"\n",
        "    window -- k_h x k_w x k_d\n",
        "    filter -- k_h x k_w x k_d\n",
        "    b      -- 1x1x1\n",
        "    Z      -- scalar\n",
        "    \"\"\"\n",
        "    H_k,W_k,D_k = filter.shape\n",
        "    out=0\n",
        "    for h in range(H_k):\n",
        "        for w in range(W_k):\n",
        "            for d in range(D_k):\n",
        "                out += similarity_f(window[h,w,d], filter[h,w,d])\n",
        "    return out\n",
        "\n",
        "class adder_layer(Layer):\n",
        "    def __init__(self,output_channels,kernel_size=3,stride=1,padding=0,bias=None,similarity_f = L1,input_channels=None):\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "\n",
        "        self.input_channels=None if not input_channels else input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # making weight instantiation optional in case we want to infer input channels from forward pass rather than defining explicitly\n",
        "        self.kernel_size=kernel_size        \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        bias = np.zeros((1,1,1,self.output_channels)) if not bias else bias\n",
        "        self.bias = bias\n",
        "        self.similarity_f = similarity_f\n",
        "\n",
        "    def forward(self,X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        filters -- c_out x k_H x k_W x c_in\n",
        "        b       -- c_out x 1 x 1 x 1\n",
        "        Z       -- n_tensors x H_new x W_new, c_out\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        self.input = X\n",
        "\n",
        "        # in case input size not given\n",
        "        self.input_channels = X.shape[-1]\n",
        "        #if self.filters==None:\n",
        "        self.filters = np.random.normal(loc=0,scale=1,size=(self.output_channels, self.kernel_size, self.kernel_size, self.input_channels))\n",
        "\n",
        "        filters,stride,padding,bias = self.filters, self.stride, self.padding, self.bias\n",
        "        n_tensors, H,   W,   c_in = X.shape\n",
        "        c_out,     H_k, W_k, c_in = filters.shape\n",
        "        n_filters = c_out\n",
        "\n",
        "        X_padded = np.pad(X, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        H_new = int((H + 2*padding - H_k)/stride)+1\n",
        "        W_new = int((W + 2*padding - W_k)/stride)+1\n",
        "\n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "\n",
        "        for i in range(n_tensors):           # traverse batch\n",
        "            this_img = X_padded[i,:,:,:]     # select ith image in batch\n",
        "            for f in range(n_filters):       # traverse filters\n",
        "                this_filter = filters[f,:,:,:]\n",
        "                #this_bias = bias[f,:,:,:]\n",
        "                for h in range(H_new):       # traverse height\n",
        "                    for w in range(W_new):   # traverse width\n",
        "                        \n",
        "                        v0,v1 = h*stride, h*stride + H_k\n",
        "                        h0,h1 = w*stride, w*stride + W_k\n",
        "                        \n",
        "                        this_window = this_img[v0:v1,h0:h1,:]\n",
        "\n",
        "                        Z[i, h, w, f] = adder_single_step(this_window, this_filter)#, this_bias) \n",
        "\n",
        "        assert Z.shape == (n_tensors, H_new, W_new, n_filters)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, filters, bias, stride, padding\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "        \n",
        "        X, filters, bias, stride, padding = self.cache\n",
        "\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        n_filters, H_k,    W_k,    c_down = filters.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "        \n",
        "        dX       = np.zeros_like(X)                           \n",
        "        dfilters = np.zeros_like(filters)\n",
        "        #dbias    = np.zeros((n_filters, 1,1,c_down))\n",
        "\n",
        "        X_padded  = np.pad(X,  ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_padded = np.pad(dX, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_padded[i]\n",
        "            \n",
        "            for h in range(H_up):                   # traverse height\n",
        "                for w in range(W_up):               # traverse width\n",
        "                    for c in range(c_up):           # traverse filters\n",
        "                        \n",
        "                        v0,v1 = h,h+H_k\n",
        "                        h0,h1 = w,w+W_k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:h1, :]\n",
        "                        f_window = filters[c,:,:,:]\n",
        "\n",
        "                        dx_local = x_window-f_window\n",
        "                        df_local = hard_tanh(f_window-x_window)\n",
        "\n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += dx_local * g\n",
        "                        dfilters[c,:,:,:]   += df_local * g\n",
        "                        #dbias[c,:,:,:]      += g\n",
        "                        \n",
        "            dX[i, :, :, :] = dx[padding:-padding, padding:-padding, :]\n",
        "        \n",
        "        assert(dX.shape == (n_tensors, H_down, W_down, c_down))\n",
        "\n",
        "        self.filters -= learning_rate*dfilters\n",
        "        #self.bias    -= learning_rate*dbias\n",
        "\n",
        "        return dX"
      ],
      "metadata": {
        "id": "CnPeNX0zDoEQ"
      },
      "execution_count": 634,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fully-connected layer"
      ],
      "metadata": {
        "id": "gVevHrit5823"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FullyConnected(Layer):\n",
        "    def __init__(self,output_channels,input_channels=None):\n",
        "        super(Layer, self).__init__()\n",
        "\n",
        "        self.input_channels=None if not input_channels else input_channels\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "        # making weight instantiation optional in case we want to infer input channels from forward pass rather than defining explicitly\n",
        "        #self.weights = None\n",
        "        #if input_channels:\n",
        "        #    self.weights = np.random.normal(loc=0,scale=1,size=(input_channels,output_channels))\n",
        "\n",
        "        self.bias = np.random.normal(loc=0,scale=1,size=(1, output_channels))\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.input_channels = X.shape[-1]\n",
        "        self.weights = np.random.normal(loc=0,scale=1,size=(self.input_channels,self.output_channels))\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        dX    = np.dot(upstream_g, self.weights.T)\n",
        "        dW    = np.dot(self.input.T, upstream_g)\n",
        "        dbias = upstream_g\n",
        "\n",
        "        self.weights -= learning_rate*dW\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "        return dX"
      ],
      "metadata": {
        "id": "Ut73yN6458Nc"
      },
      "execution_count": 622,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flatten layer"
      ],
      "metadata": {
        "id": "wuAVweO_Apj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, X):\n",
        "        self.original_shape = X.shape\n",
        "        self.output = X.reshape(X.shape[0],np.product(X.shape[1:]))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        return self.upstream_g.reshape(self.original_shape)"
      ],
      "metadata": {
        "id": "J_v6tBkaArdE"
      },
      "execution_count": 623,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BatchNorm Layer"
      ],
      "metadata": {
        "id": "RjpocIPJHz-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class batch_norm_layer(Layer):\n",
        "    def __init__(self, input_channels=None, gamma=None,beta=None):\n",
        "    # def __init__(self, input_channels, gamma=None, beta=None):\n",
        "\n",
        "        '''I think we need n_tensors rather than input_channels for gamma and beta dimensions'''\n",
        "#        self.input_channels = input_channels\n",
        "        \n",
        "        # gamma = np.zeros((self.input_channels,1,1,1)) if not gamma else gamma\n",
        "        gamma = None if not gamma else gamma\n",
        "        self.gamma = gamma\n",
        "        # beta = np.ones((self.input_channels,1,1,1)) if not beta else beta\n",
        "        beta = None if not beta else beta\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        gamma   -- n_tensors x 1 x 1 x 1\n",
        "        beta    -- n_tensors x 1 x 1 x 1\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "\n",
        "        self.input = X\n",
        "        gamma = np.zeros((X.shape[0],1,1,1)) if self.gamma==None else self.gamma\n",
        "        beta = np.ones((X.shape[0],1,1,1)) if self.beta==None else self.beta\n",
        "\n",
        "        #gamma, beta = self.gamma, self.beta\n",
        "        \n",
        "        mean = np.mean(X, axis=0)\n",
        "        std = np.std(X, axis=0)\n",
        "        X_center = X - mean\n",
        "        X_norm = X_center/std\n",
        "\n",
        "        self.output = gamma*X_norm + beta\n",
        "        \n",
        "        self.cache = X, X_center, X_norm\n",
        "\n",
        "        return self.output \n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, X_norm)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "\n",
        "        X, X_norm = self.cache\n",
        "\n",
        "        dGamma = np.sum(upstream_g * X_norm, axis=0)\n",
        "        dBeta = np.sum(upstream_g, axis=0)\n",
        "\n",
        "        m = len(X)\n",
        "        mean = np.mean(X)\n",
        "        std = np.std(X)\n",
        "        \n",
        "        dX = np.zeros_like(X)\n",
        "\n",
        "        for i in range(m):\n",
        "            for j in range(m):\n",
        "                dX[i] += (upstream_g[i] - upstream_g[j]*(1 + (X[i]-X[j])*(X[j]-mean)/std))\n",
        "            dX[i] *= self.gamma/((m**2)*std)\n",
        "        \n",
        "        self.gamma -= learning_rate*dGamma\n",
        "        self.beta -= learning_rate*dBeta\n",
        "\n",
        "        return dX"
      ],
      "metadata": {
        "id": "kpzQWoTFH1rv"
      },
      "execution_count": 660,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maxpool layer"
      ],
      "metadata": {
        "id": "D2t7dqw4ngiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool(Layer):\n",
        "    def __init__(self,pool_size=2,stride=1):\n",
        "        self.pool_size=pool_size\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self,X):\n",
        "        n_tensors, H, W, c_in = X.shape\n",
        "\n",
        "        H_new = int(1 + (H - self.pool_size) / self.stride)\n",
        "        W_new = int(1 + (W - self.pool_size) / self.stride)\n",
        "        c_out = c_in\n",
        "        \n",
        "        Z = np.zeros((n_tensors, H_new, W_new, c_out))              \n",
        "        \n",
        "        for i in range(n_tensors):                     # loop over the training examples\n",
        "            for h in range(H_new):                     # loop on the vertical axis of the output volume\n",
        "                for w in range(W_new):                 # loop on the horizontal axis of the output volume\n",
        "                    for c in range(c_out):             # loop over the channels of the output volume\n",
        "                        \n",
        "                        v0,v1 = h*self.stride, h*self.stride + self.pool_size\n",
        "                        h0,h1 = w*self.stride, w*self.stride + self.pool_size\n",
        "                        \n",
        "                        window = X[i, v0:v1, h0:h1,c]\n",
        "                    \n",
        "                        Z[i, h, w, c] = np.max(window)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, self.pool_size, self.stride\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g,learning_rate):\n",
        "        X, pool_size, stride = self.cache\n",
        "        \n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "\n",
        "        dX = np.zeros(X.shape)\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X[i]\n",
        "            for h in range(H_up):                   \n",
        "                for w in range(W_up):             \n",
        "                    for c in range(c_up):           \n",
        "                        v0,v1 = h, h+pool_size\n",
        "                        h0,h1 = w, w+pool_size\n",
        "\n",
        "                        x_window = x[v0:v1, h0:h1, c]\n",
        "                        \n",
        "                        local_g = np.where(x_window==np.max(x_window))\n",
        "                        g       = upstream_g[i, h, w, c]\n",
        "                         \n",
        "                        dX[i, v0:v1, h0:h1, c] += local_g * g\n",
        "\n",
        "        assert(dX.shape == X.shape)\n",
        "        \n",
        "        return dX\n"
      ],
      "metadata": {
        "id": "_zJEQA1fAzVS"
      },
      "execution_count": 624,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Activation layers"
      ],
      "metadata": {
        "id": "RO7MWk0_B3XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_fwd(X):\n",
        "    return np.where(X>=0,X,0)\n",
        "def relu_bwd(X):\n",
        "    return np.where(X>=0,1,0)\n",
        "def softmax_fwd(X):\n",
        "    x_max = np.amax(X, axis=0, keepdims=True)\n",
        "    exp_ = np.exp(X - x_max)\n",
        "    return exp_ / np.sum(exp_, axis=1, keepdims=True)\n",
        "def softmax_bwd(X):\n",
        "    I = np.eye(X.shape[0])\n",
        "    s = softmax_fwd(X)\n",
        "    return s*(I - s.T)\n",
        "def sig_fwd(X):\n",
        "    #return tf.math.sigmoid(X)\n",
        "    return 1/(1 + np.exp(-X))\n",
        "def sig_bwd(X):\n",
        "    return sig_fwd(X) * (1 - sig_fwd(X))\n",
        "\n",
        "\n",
        "activation_dict = {'relu':    {'forward':  relu_fwd,\n",
        "                               'backward': relu_bwd},\n",
        "                   'softmax': {'forward':  softmax_fwd,\n",
        "                               'backward': softmax_bwd},\n",
        "                   'sigmoid': {'forward':  sig_fwd,\n",
        "                               'backward': sig_bwd}}\n",
        "\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self,activation_name):\n",
        "        super(Layer, self).__init__()\n",
        "        self.fwd=activation_dict[activation_name]['forward']\n",
        "        self.bwd=activation_dict[activation_name]['backward']\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.input = X\n",
        "        self.output = self.fwd(X)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        local_g = self.bwd(self.input)\n",
        "        return local_g*upstream_g"
      ],
      "metadata": {
        "id": "jLa7JtufB6Tr"
      },
      "execution_count": 625,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `Model` class"
      ],
      "metadata": {
        "id": "qKx4NGICC-74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(y_true, y_pred):    \n",
        "    log_likelihood =  y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)\n",
        "    return -np.mean(log_likelihood)\n",
        "\n",
        "def binary_cross_entropy_prime(y_true,y_pred):\n",
        "    return np.mean(-y_true/y_pred + (1-y_true)/(1-y_pred))\n",
        "\n",
        "\n",
        "loss_dict = {'binary_cross_entropy': {'forward':  binary_cross_entropy,\n",
        "                                      'backward': binary_cross_entropy_prime}}\n",
        "\n",
        "\n",
        "\n",
        "def single_example_accuracy(y_real,y_pred):\n",
        "    if y_pred.shape[-1] == 1: # i.e. if not one-hot encoded\n",
        "        if np.max(y_pred)>1:  # multi-category integer labels\n",
        "            pass\n",
        "        else:\n",
        "            y_real=1 if y_real.astype(float)>=0.5 else 0\n",
        "            y_pred=1 if y_pred.astype(float)>=0.5 else 0\n",
        "    else:\n",
        "        y_real = np.argmax(y_real)\n",
        "        y_pred = np.argmax(y_pred)\n",
        "    \n",
        "    return 1 if y_real == y_pred else 0\n",
        "\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,loss_name): # planning on creating a dictionary so we can get a loss function (forward + backward) from its name\n",
        "        self.layers = []\n",
        "        self.loss_fwd = loss_dict[loss_name]['forward']\n",
        "        self.loss_bwd = loss_dict[loss_name]['backward']\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        y_hat = []\n",
        "\n",
        "        #for i in range(input_data.shape[0]):\n",
        "        Z = input_data\n",
        "        for layer in self.layers:\n",
        "            Z = layer.forward(Z)\n",
        "        y_hat = Z\n",
        "\n",
        "        return y_hat\n",
        "\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate, x_val=None, y_val=None):\n",
        "\n",
        "        history = {'accuracy': [],\n",
        "                   'loss': [],\n",
        "                   'val_accuracy': [],\n",
        "                   'val_loss': []}\n",
        "\n",
        "        for e in range(epochs):\n",
        "            print(e)\n",
        "            loss = 0\n",
        "            acc  = 0\n",
        "            val_loss = 0\n",
        "            val_acc = 0\n",
        "            #for j in range(x_train.shape[0]):\n",
        "\n",
        "            # forward\n",
        "            Z = x_train#[j]\n",
        "            for layer in self.layers:\n",
        "                print(layer)\n",
        "                Z = layer.forward(Z)\n",
        "\n",
        "            y_real = y_train#[j]\n",
        "            y_pred = Z\n",
        "\n",
        "            # compute loss and accuracy\n",
        "            loss += self.loss_fwd(y_real, y_pred)\n",
        "            acc  += single_example_accuracy(y_real, y_pred)\n",
        "\n",
        "            # backwward\n",
        "            error = self.loss_bwd(y_real, y_pred)\n",
        "            for layer in (self.layers)[::-1]:\n",
        "                error = layer.backward(error, learning_rate)\n",
        "\n",
        "\n",
        "            loss /= x_train.shape[0]\n",
        "            acc  /= x_train.shape[0]\n",
        "            history['accuracy'].append(acc)\n",
        "            history['loss'].append(loss)\n",
        "\n",
        "            if not x_val or not y_val:\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss,3))}   acc = {str(round(acc,3))}')\n",
        "\n",
        "            elif x_val and y_val:\n",
        "                for k in range(x_val.shape[0]):\n",
        "                    Z_val = x_val[j]\n",
        "                    for layer in self.layers:\n",
        "                        Z_val = layer.forward(Z_val)\n",
        "\n",
        "                    y_real_val = y_val[k]\n",
        "                    y_pred_val = Z_val\n",
        "\n",
        "                    val_loss += self.loss_fwd(y_real_val, y_pred_val)\n",
        "                    val_acc  += single_example_accuracy(y_real_val, y_pred_val)\n",
        "\n",
        "                history['val_accuracy'].append(val_acc)\n",
        "                history['val_loss'].append(val_loss)\n",
        "\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss,3))}   acc = {str(round(acc,3))}   val_loss = {str(round(val_loss,3))}   val_accuracy = {str(round(val_acc,3))}')\n"
      ],
      "metadata": {
        "id": "iaEag2IkC-JR"
      },
      "execution_count": 626,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data collection\n",
        "\n",
        "### p.s. - been having a hard time finding an imagenet dataset/subset that is a reasonable size and easy to download"
      ],
      "metadata": {
        "id": "cbHmgJRtCj69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CIFAR10"
      ],
      "metadata": {
        "id": "8Oo5KOmoCzNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "import pickle\n",
        "import tensorflow.keras.utils as np_utils\n",
        "\n",
        "\n",
        "# !wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "# data_zip = os.path.join(root,'cifar-10-python.tar.gz')\n",
        "# f = tarfile.open(data_zip)\n",
        "# f.extractall(root) \n",
        "# f.close()\n",
        "# os.remove(data_zip)\n",
        "\n",
        "def load_cifar_data(folder,tiny=False):\n",
        "    train_batches = [f'{folder}/{f}' for f in os.listdir(folder) if 'batch_' in f]\n",
        "    test_batch    =  f'{folder}/test_batch'\n",
        "\n",
        "    # Get train data\n",
        "    X_trn = None\n",
        "    y_trn = []\n",
        "    for i in range(len(train_batches)):\n",
        "        train_data_dict = pickle.load(open(train_batches[i],'rb'), encoding='latin-1')\n",
        "        if i+1 == 1:\n",
        "            X_trn = train_data_dict['data']\n",
        "        else:\n",
        "            X_trn = np.vstack((X_trn, train_data_dict['data']))\n",
        "        y_trn += train_data_dict['labels']\n",
        "    X_trn = X_trn.reshape(len(X_trn),3,32,32)\n",
        "    X_trn = np.rollaxis(X_trn,1,4)\n",
        "    X_trn = X_trn.astype('float32')/255.0\n",
        "    y_trn = np_utils.to_categorical(np.asarray(y_trn),10)\n",
        "\n",
        "    # Get test data\n",
        "    test_data_dict  = pickle.load(open(test_batch,'rb'), encoding='latin-1')\n",
        "    X_tst = test_data_dict['data']\n",
        "    X_tst = X_tst.reshape(len(X_tst),3,32,32)\n",
        "    X_tst = np.rollaxis(X_tst,1,4)\n",
        "    X_tst = X_tst.astype('float32')/255.0\n",
        "    y_tst = np_utils.to_categorical(np.asarray(test_data_dict['labels']))\n",
        "    \n",
        "    n_90 = int(0.9*len(X_trn))\n",
        "    X_trn, X_val = X_trn[:n_90], X_trn[n_90:]\n",
        "    y_trn, y_val = y_trn[:n_90], y_trn[n_90:]\n",
        "\n",
        "    if tiny:\n",
        "        X_trn,y_trn,X_tst,y_tst,X_val,y_val = X_trn[:10],y_trn[:10],X_tst[:5],y_tst[:5],X_val[:5],y_val[:5]\n",
        "\n",
        "    return X_trn, y_trn, X_tst, y_tst, X_val, y_val\n",
        "\n",
        "data_dir = f'{root}/cifar-10-batches-py'\n",
        "X_trn_c10, y_trn_c10, X_tst_c10, y_tst_c10, X_val_c10, y_val_c10 = load_cifar_data(data_dir,tiny=True)"
      ],
      "metadata": {
        "id": "uHl1tum3l-oL"
      },
      "execution_count": 627,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "this_model = Model(loss_name='binary_cross_entropy')\n",
        "\n",
        "this_model.add(adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1))\n",
        "this_model.add(Activation('relu'))\n",
        "this_model.add(MaxPool(pool_size=2))\n",
        "this_model.add(batch_norm_layer(16))\n",
        "\n",
        "# this_model.add(adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1))\n",
        "# this_model.add(Activation('relu'))\n",
        "# this_model.add(MaxPool(pool_size=2))\n",
        "\n",
        "# this_model.add(adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1))\n",
        "# this_model.add(Activation('relu'))\n",
        "# this_model.add(MaxPool(pool_size=2))\n",
        "\n",
        "this_model.add(Flatten())\n",
        "\n",
        "this_model.add(FullyConnected(output_channels=64))\n",
        "this_model.add(Activation('relu'))\n",
        "this_model.add(FullyConnected(output_channels=10))\n",
        "this_model.add(Activation('softmax'))"
      ],
      "metadata": {
        "id": "kRIJNY0dRvD2"
      },
      "execution_count": 661,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "this_model.fit(X_trn_c10, y_trn_c10, 10, 1e-05, x_val=X_val_c10, y_val=y_val_c10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "eBl1GKVe9MCg",
        "outputId": "2b483949-fb0c-4478-bc63-1ea1ec1f4c63"
      },
      "execution_count": 662,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "<__main__.adder_layer object at 0x7f183c18d710>\n",
            "<__main__.Activation object at 0x7f183c18df90>\n",
            "<__main__.MaxPool object at 0x7f183c18d450>\n",
            "<__main__.batch_norm_layer object at 0x7f183c18de10>\n",
            "<__main__.Flatten object at 0x7f183c18de50>\n",
            "<__main__.FullyConnected object at 0x7f183c18dcd0>\n",
            "<__main__.Activation object at 0x7f183c229a50>\n",
            "<__main__.FullyConnected object at 0x7f183c27e410>\n",
            "<__main__.Activation object at 0x7f183cc79550>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-662-299f3cdc13e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mthis_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val_c10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val_c10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-626-9f86c1984e08>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epochs, learning_rate, x_val, y_val)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_bwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-622-9bf7f2dfcb8c>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, upstream_g, learning_rate)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m    \u001b[0;34m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (1,10) doesn't match the broadcast shape (10,10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adder1 = adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1)"
      ],
      "metadata": {
        "id": "RDgzyFaH9Ykb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bn = batch_norm_layer()\n",
        "\n",
        "bn.forward(X_trn_c10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iT-VYgVs_vbJ",
        "outputId": "51213352-339e-4840-8946-d1a29694ee6e"
      },
      "execution_count": 659,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n",
            "b\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 659
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.rand(10,32,32,3) + np.random.rand(10,1,1,1)"
      ],
      "metadata": {
        "id": "qkxN--rDIdE9",
        "outputId": "c6f90e01-35ea-496f-f94f-bd71a9681257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 655,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.79440083, 1.02186662, 1.26636492],\n",
              "         [1.43716924, 1.30347621, 0.88450701],\n",
              "         [1.02146461, 1.05553452, 0.52918071],\n",
              "         ...,\n",
              "         [1.25315822, 0.81705265, 0.49885102],\n",
              "         [1.03179866, 0.82110035, 1.05254131],\n",
              "         [0.95010404, 1.37250639, 1.45331973]],\n",
              "\n",
              "        [[0.62792069, 0.91742906, 1.17211183],\n",
              "         [1.29757296, 0.52731206, 1.15776192],\n",
              "         [1.30814221, 0.54906531, 0.93007028],\n",
              "         ...,\n",
              "         [1.01001394, 0.72603325, 0.9807498 ],\n",
              "         [1.23080567, 0.68434759, 0.96854027],\n",
              "         [1.05486206, 1.41987464, 1.39319223]],\n",
              "\n",
              "        [[1.28312694, 1.27648897, 1.01577631],\n",
              "         [0.89668966, 0.59219371, 0.577873  ],\n",
              "         [1.14378208, 1.20014866, 0.63642031],\n",
              "         ...,\n",
              "         [0.83492402, 1.15127163, 1.45335143],\n",
              "         [1.00628706, 0.86233897, 1.20181766],\n",
              "         [1.30943711, 1.15298078, 0.57270456]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1.31878556, 1.29869075, 1.44200988],\n",
              "         [1.28292923, 0.50326362, 1.15365078],\n",
              "         [0.97331443, 0.5092365 , 1.13379711],\n",
              "         ...,\n",
              "         [0.51811676, 0.98113542, 0.93405981],\n",
              "         [0.79040126, 1.00236287, 0.66461815],\n",
              "         [0.47293045, 0.84114019, 0.74223999]],\n",
              "\n",
              "        [[0.81235374, 1.32158847, 0.65692499],\n",
              "         [1.12298533, 0.79115948, 0.53535726],\n",
              "         [0.79478606, 1.11863691, 0.89340932],\n",
              "         ...,\n",
              "         [0.95716736, 1.1023953 , 1.4436352 ],\n",
              "         [0.63858548, 1.1712445 , 0.95376842],\n",
              "         [1.01787575, 1.41792543, 0.93976837]],\n",
              "\n",
              "        [[1.08346955, 1.45621764, 0.61156511],\n",
              "         [0.82808535, 0.72162156, 1.10123679],\n",
              "         [1.23154272, 1.00232342, 1.03257533],\n",
              "         ...,\n",
              "         [0.79077089, 0.55466344, 1.39459023],\n",
              "         [0.92605106, 1.06409417, 1.27853882],\n",
              "         [0.68087786, 1.14061015, 1.24855063]]],\n",
              "\n",
              "\n",
              "       [[[1.275962  , 0.72624435, 0.94306118],\n",
              "         [1.25269528, 1.04600048, 1.3004628 ],\n",
              "         [0.65645803, 1.3923777 , 1.25852709],\n",
              "         ...,\n",
              "         [1.08481662, 1.09918533, 1.01921086],\n",
              "         [0.90058341, 0.97711049, 1.00099085],\n",
              "         [1.41456234, 1.28699775, 1.30232965]],\n",
              "\n",
              "        [[0.78584712, 1.06377748, 1.44423276],\n",
              "         [0.69915538, 1.31152382, 1.46329228],\n",
              "         [0.56209421, 0.75470312, 0.96549574],\n",
              "         ...,\n",
              "         [1.31659661, 1.08459249, 1.01004296],\n",
              "         [1.06254241, 1.35088666, 0.56123972],\n",
              "         [1.30817156, 1.28272127, 0.92190565]],\n",
              "\n",
              "        [[1.24958802, 0.81743526, 0.88626655],\n",
              "         [0.69550525, 1.49433449, 0.56107259],\n",
              "         [1.24585649, 1.34997632, 0.87006621],\n",
              "         ...,\n",
              "         [1.17311539, 1.05041719, 1.47984286],\n",
              "         [0.95049219, 0.56902538, 0.69639356],\n",
              "         [1.34924809, 0.57728641, 1.30849641]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1.35159022, 0.99489143, 1.29094968],\n",
              "         [0.73039211, 1.12523867, 0.51033605],\n",
              "         [0.81807252, 1.37311602, 0.92510547],\n",
              "         ...,\n",
              "         [0.66316744, 0.70714282, 0.65231676],\n",
              "         [0.59793294, 0.81070876, 1.15483258],\n",
              "         [0.51729735, 0.68372056, 1.4375546 ]],\n",
              "\n",
              "        [[0.77027997, 1.38901675, 1.28785861],\n",
              "         [0.56170704, 1.28634138, 1.02835394],\n",
              "         [0.51946612, 1.4107634 , 1.17248498],\n",
              "         ...,\n",
              "         [1.48743189, 0.73655431, 1.14988674],\n",
              "         [1.21707768, 0.55078011, 1.17188252],\n",
              "         [1.08591079, 0.85823125, 0.72613173]],\n",
              "\n",
              "        [[0.67098636, 1.2509572 , 1.00334052],\n",
              "         [1.17830596, 1.09424424, 0.98779856],\n",
              "         [0.85976992, 0.64257022, 0.70604429],\n",
              "         ...,\n",
              "         [0.8707799 , 1.38583882, 0.69677563],\n",
              "         [1.2559901 , 1.35672952, 1.39005729],\n",
              "         [1.06831437, 0.55064402, 1.0439433 ]]],\n",
              "\n",
              "\n",
              "       [[[1.39118199, 1.58497287, 1.73221493],\n",
              "         [1.56127313, 1.43122936, 1.07217157],\n",
              "         [1.17701413, 1.08865115, 1.07427007],\n",
              "         ...,\n",
              "         [1.58382635, 1.05111031, 0.91765168],\n",
              "         [1.47174351, 1.33860368, 1.62122867],\n",
              "         [1.67570079, 0.90605832, 1.726915  ]],\n",
              "\n",
              "        [[1.48651262, 1.15093668, 0.82002901],\n",
              "         [1.43687619, 1.30511439, 1.72549268],\n",
              "         [1.15158175, 1.193517  , 1.6272314 ],\n",
              "         ...,\n",
              "         [1.20259023, 0.85245961, 1.41260465],\n",
              "         [0.92839642, 1.72203232, 1.08447077],\n",
              "         [0.85690126, 1.32201066, 1.13454222]],\n",
              "\n",
              "        [[0.81811219, 1.43862667, 0.84751796],\n",
              "         [1.24968339, 1.59342472, 1.63942899],\n",
              "         [1.0911054 , 1.16492504, 0.89269752],\n",
              "         ...,\n",
              "         [1.03860238, 1.123901  , 1.10381496],\n",
              "         [1.35137603, 0.85062405, 1.70408167],\n",
              "         [1.57094126, 1.7353521 , 0.9649108 ]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1.62423202, 1.28118175, 1.29936929],\n",
              "         [0.85711097, 0.92658595, 1.1694544 ],\n",
              "         [0.98024052, 1.41788472, 1.77841615],\n",
              "         ...,\n",
              "         [1.03391598, 1.40703197, 0.83450193],\n",
              "         [1.79934244, 1.01594795, 1.59346011],\n",
              "         [1.49793753, 1.583573  , 1.50413369]],\n",
              "\n",
              "        [[1.13632165, 1.18155328, 1.6584806 ],\n",
              "         [0.97918094, 1.75443482, 1.42036998],\n",
              "         [1.3190724 , 1.72100077, 0.92066725],\n",
              "         ...,\n",
              "         [1.19884241, 1.70995389, 1.69132407],\n",
              "         [1.01705316, 1.44738554, 1.6708696 ],\n",
              "         [1.50093126, 1.35994985, 0.91211951]],\n",
              "\n",
              "        [[0.96171007, 1.29627026, 1.70632895],\n",
              "         [1.10734776, 1.45908751, 1.69094544],\n",
              "         [1.47981187, 0.97904846, 1.16412776],\n",
              "         ...,\n",
              "         [1.62809404, 1.05610471, 0.85468516],\n",
              "         [1.09067986, 0.98233218, 1.41757747],\n",
              "         [1.45814583, 1.28672776, 0.94396463]]],\n",
              "\n",
              "\n",
              "       ...,\n",
              "\n",
              "\n",
              "       [[[1.36457402, 1.58356521, 1.18639129],\n",
              "         [1.87592986, 1.52041795, 1.57889211],\n",
              "         [1.06176949, 1.02761776, 1.28100063],\n",
              "         ...,\n",
              "         [1.39491382, 1.67108609, 1.91586817],\n",
              "         [1.22439803, 0.98756619, 1.27879079],\n",
              "         [1.23340764, 1.88460652, 1.49667056]],\n",
              "\n",
              "        [[1.60822967, 1.27739978, 1.33301234],\n",
              "         [1.01669991, 1.56061421, 1.20477002],\n",
              "         [1.56166431, 1.26433544, 1.87814109],\n",
              "         ...,\n",
              "         [1.10175783, 1.00386553, 0.95950108],\n",
              "         [1.05175045, 1.2050184 , 1.37778737],\n",
              "         [1.22695386, 1.48363269, 1.57374843]],\n",
              "\n",
              "        [[1.18852416, 1.8027996 , 0.95493377],\n",
              "         [1.65370935, 1.78532953, 1.30214796],\n",
              "         [1.72947099, 1.53950807, 1.69039111],\n",
              "         ...,\n",
              "         [1.60153188, 1.75027942, 1.83958459],\n",
              "         [1.85077505, 1.84412902, 1.39949925],\n",
              "         [1.84033504, 1.44925032, 1.55945088]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1.2837716 , 1.21018958, 1.41953372],\n",
              "         [1.79197829, 1.58500265, 1.94573155],\n",
              "         [1.90379634, 1.3990181 , 1.40200766],\n",
              "         ...,\n",
              "         [1.85140535, 1.15321984, 1.57328109],\n",
              "         [1.83919587, 1.86683146, 1.58368141],\n",
              "         [1.29401343, 1.69971426, 1.23222315]],\n",
              "\n",
              "        [[1.55877013, 1.67251917, 1.62276534],\n",
              "         [1.38735459, 1.7581429 , 1.45779767],\n",
              "         [1.61053747, 1.86941454, 1.53002504],\n",
              "         ...,\n",
              "         [1.27585283, 1.63228086, 1.58164251],\n",
              "         [1.60691626, 1.18584872, 1.2881046 ],\n",
              "         [0.96774656, 1.26316267, 1.59128868]],\n",
              "\n",
              "        [[1.5834401 , 1.02767304, 1.45666752],\n",
              "         [1.92004251, 1.53807025, 1.554231  ],\n",
              "         [0.97521151, 1.30740839, 1.80585925],\n",
              "         ...,\n",
              "         [1.90569842, 1.45788602, 1.12561695],\n",
              "         [1.63249394, 1.4985066 , 1.34158248],\n",
              "         [1.32673951, 1.28894429, 1.43525712]]],\n",
              "\n",
              "\n",
              "       [[[1.47820631, 1.77623783, 1.52457955],\n",
              "         [1.69782501, 1.56268546, 1.42055147],\n",
              "         [1.49155687, 1.4738935 , 1.49651003],\n",
              "         ...,\n",
              "         [0.85193248, 1.63501913, 1.39576137],\n",
              "         [1.67532337, 1.09537196, 1.23052146],\n",
              "         [1.47349498, 0.90434423, 1.10222482]],\n",
              "\n",
              "        [[0.99034205, 1.01303943, 1.54047016],\n",
              "         [1.02620566, 0.96853164, 0.83926244],\n",
              "         [1.73156143, 1.37225506, 1.66581354],\n",
              "         ...,\n",
              "         [1.38976322, 1.05123963, 1.26603624],\n",
              "         [0.92804565, 1.64339835, 1.41822043],\n",
              "         [0.84073429, 0.99849835, 0.95141629]],\n",
              "\n",
              "        [[1.52312653, 1.32056256, 1.49904678],\n",
              "         [1.6469595 , 1.61729723, 1.52305744],\n",
              "         [1.43731325, 1.54468945, 1.6425919 ],\n",
              "         ...,\n",
              "         [1.24593187, 1.40665371, 1.58529617],\n",
              "         [1.78209604, 1.00381877, 1.2141047 ],\n",
              "         [1.55980646, 1.1691828 , 1.28375203]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[0.82795654, 1.38265281, 1.68604303],\n",
              "         [1.55948704, 1.39282629, 1.00449872],\n",
              "         [1.65890405, 1.03942907, 1.77036424],\n",
              "         ...,\n",
              "         [0.85785791, 1.14912462, 1.7071673 ],\n",
              "         [1.55183983, 1.7809487 , 1.65658806],\n",
              "         [1.05453295, 1.37960308, 0.94597014]],\n",
              "\n",
              "        [[1.42799652, 0.93585943, 0.99639875],\n",
              "         [1.03028108, 1.37812267, 1.54206085],\n",
              "         [0.93511563, 1.09062563, 1.28967589],\n",
              "         ...,\n",
              "         [1.04539804, 1.00194169, 1.2980174 ],\n",
              "         [1.50779823, 1.50480197, 1.03377617],\n",
              "         [1.21243536, 1.2251981 , 1.6812727 ]],\n",
              "\n",
              "        [[0.9925709 , 1.48047313, 0.95504295],\n",
              "         [1.25427331, 1.71978139, 1.74668599],\n",
              "         [1.35406325, 1.46119113, 0.85555552],\n",
              "         ...,\n",
              "         [1.19294374, 1.68787741, 1.07444535],\n",
              "         [1.19763837, 0.80567999, 1.60720866],\n",
              "         [1.0527024 , 1.42988804, 1.58092725]]],\n",
              "\n",
              "\n",
              "       [[[1.25135241, 1.22027047, 1.17489362],\n",
              "         [1.64211173, 1.48800391, 1.40943432],\n",
              "         [0.9004571 , 1.33669619, 1.0043346 ],\n",
              "         ...,\n",
              "         [1.08408919, 1.54367576, 0.99518282],\n",
              "         [1.31543114, 1.00005715, 1.68145324],\n",
              "         [1.16020259, 1.70853656, 0.78001321]],\n",
              "\n",
              "        [[1.70424857, 0.71980497, 1.31746548],\n",
              "         [1.45030103, 1.48495849, 1.49820214],\n",
              "         [0.9048733 , 1.60271084, 1.50278768],\n",
              "         ...,\n",
              "         [1.61036338, 1.51039937, 1.62432567],\n",
              "         [1.65165247, 1.54979936, 0.93511397],\n",
              "         [0.96947673, 1.0169431 , 1.03986488]],\n",
              "\n",
              "        [[1.58229861, 1.39201901, 0.98333145],\n",
              "         [1.71502461, 1.63045797, 1.56837357],\n",
              "         [1.57888635, 1.23025763, 0.72505814],\n",
              "         ...,\n",
              "         [1.42074203, 1.46906696, 0.9713298 ],\n",
              "         [1.48459737, 0.92441614, 1.58645412],\n",
              "         [1.48476993, 1.44904311, 1.41014013]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1.56048746, 1.6284766 , 1.54080176],\n",
              "         [1.35578073, 0.78432114, 0.77849655],\n",
              "         [1.64513577, 1.2654593 , 0.98578147],\n",
              "         ...,\n",
              "         [0.92825649, 0.81463799, 1.00844924],\n",
              "         [1.18174052, 0.87061227, 1.26685659],\n",
              "         [1.12778474, 1.61198203, 1.33534717]],\n",
              "\n",
              "        [[0.81917131, 0.99005229, 0.89487983],\n",
              "         [1.51265224, 1.02695494, 1.29483964],\n",
              "         [0.90101683, 0.9708921 , 1.29084168],\n",
              "         ...,\n",
              "         [1.52528994, 0.78627531, 1.62689865],\n",
              "         [1.55635155, 0.85678303, 1.10925344],\n",
              "         [1.14393797, 1.08984725, 1.15068539]],\n",
              "\n",
              "        [[1.12263332, 1.23878001, 1.55621036],\n",
              "         [0.86196512, 1.3041653 , 1.10576973],\n",
              "         [0.96891968, 0.9651614 , 1.61874409],\n",
              "         ...,\n",
              "         [0.80077249, 1.02415285, 1.35051651],\n",
              "         [1.6676863 , 1.1694523 , 1.36923752],\n",
              "         [1.24459969, 0.94940415, 1.07984107]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 655
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rxPxz7rnMl50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}