{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xJBCiQ6eoTki"
      },
      "outputs": [],
      "source": [
        "from data import load_cifar_data\n",
        "from model import Model\n",
        "import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uHl1tum3l-oL"
      },
      "outputs": [],
      "source": [
        "data_dir = f'{root}/cifar-10-batches-py'\n",
        "X_trn_c10, y_trn_c10, X_tst_c10, y_tst_c10, X_val_c10, y_val_c10 = load_cifar_data(data_dir,tiny=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kRIJNY0dRvD2"
      },
      "outputs": [],
      "source": [
        "this_model = Model(loss_name='cat_cross_entropy')\n",
        "\n",
        "this_model.add(layers.adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1,adaptive_eta=0.1))\n",
        "this_model.add(layers.Activation('relu'))\n",
        "this_model.add(layers.MaxPool(pool_size=2))\n",
        "this_model.add(layers.batch_norm_layer())\n",
        "\n",
        "\n",
        "this_model.add(layers.Flatten())\n",
        "this_model.add(layers.FullyConnected(output_channels=64))\n",
        "this_model.add(layers.Activation('relu'))\n",
        "this_model.add(layers.FullyConnected(output_channels=10))\n",
        "this_model.add(layers.Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBl1GKVe9MCg",
        "outputId": "4f614581-c567-4985-a106-bb05a04a9bd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Processing Batch 0/63\r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-16 19:19:16.779092: E tensorflow/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
            "2022-11-16 19:19:16.779126: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: quinlan-bock\n",
            "2022-11-16 19:19:16.779131: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: quinlan-bock\n",
            "2022-11-16 19:19:16.779277: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 515.76.0\n",
            "2022-11-16 19:19:16.779291: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 515.76.0\n",
            "2022-11-16 19:19:16.779294: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 515.76.0\n",
            "2022-11-16 19:19:16.779684: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing Batch 5/63\r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m this_model\u001b[39m.\u001b[39;49mfit(X_trn_c10,y_trn_c10,epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,learning_rate\u001b[39m=\u001b[39;49m\u001b[39m1e-05\u001b[39;49m,x_val\u001b[39m=\u001b[39;49mX_val_c10,y_val\u001b[39m=\u001b[39;49my_val_c10)\n",
            "File \u001b[0;32m~/Desktop/IIT/CS577-Deep-Learning/Final Project/cs577_addernet/model.py:56\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x_train, y_train, epochs, batch_size, learning_rate, x_val, y_val)\u001b[0m\n\u001b[1;32m     53\u001b[0m     error \u001b[39m=\u001b[39m \u001b[39m-\u001b[39my\u001b[39m/\u001b[39m(np\u001b[39m.\u001b[39margmax(y_pred,axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m util\u001b[39m.\u001b[39meps())\n\u001b[1;32m     55\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]:\n\u001b[0;32m---> 56\u001b[0m         error \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(error, learning_rate)\n\u001b[1;32m     58\u001b[0m loss_ \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     59\u001b[0m acc  \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m x_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/Desktop/IIT/CS577-Deep-Learning/Final Project/cs577_addernet/layers.py:311\u001b[0m, in \u001b[0;36mMaxPool.backward\u001b[0;34m(self, upstream_g, learning_rate)\u001b[0m\n\u001b[1;32m    307\u001b[0m h0,h1 \u001b[39m=\u001b[39m w, w\u001b[39m+\u001b[39mpool_size\n\u001b[1;32m    309\u001b[0m x_window \u001b[39m=\u001b[39m x[v0:v1, h0:h1, c]\n\u001b[0;32m--> 311\u001b[0m local_g \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(x_window\u001b[39m==\u001b[39;49mnp\u001b[39m.\u001b[39;49mmax(x_window),\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m)\n\u001b[1;32m    312\u001b[0m \u001b[39m# single images:\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# g = upstream_g[h,w,c] \u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m# batches: \u001b[39;00m\n\u001b[1;32m    315\u001b[0m g \u001b[39m=\u001b[39m upstream_g[i, h, w, c]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = this_model.fit(X_trn_c10,y_trn_c10,epochs=20,batch_size=16,learning_rate=1e-05,x_val=X_val_c10,y_val=y_val_c10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI4_IdBIqhNy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(20),history['accuracy'],label='train')\n",
        "plt.plot(range(20),history['val_accuracy'],label='val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.xticks(range(0,21,2))\n",
        "plt.ylim(0,0.3)\n",
        "plt.title('Accuracy -- our AdderNet implementation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CDP87ePA6ji"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(20),history['loss'],label='train')\n",
        "plt.plot(range(20),history['val_loss'],label='val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.xticks(range(0,21,2))\n",
        "plt.ylim(0,0.1)\n",
        "plt.title('Loss -- our AdderNet implementation')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8_z0zzAqNIK"
      },
      "outputs": [],
      "source": [
        "#this_model = Model(loss_name='cat_cross_entropy')\n",
        "# l1 = X_trn_c10\n",
        "# l1=adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1).forward(l1)\n",
        "# print(l1.shape)\n",
        "# l2=Activation('relu').forward(l1)\n",
        "# print(l2.shape)\n",
        "# l3=MaxPool(pool_size=2).forward(l2)\n",
        "# print(l3.shape)\n",
        "# l4=batch_norm_layer().forward(l3)\n",
        "# print(l4.shape)\n",
        "\n",
        "# l5=Flatten().forward(l4)\n",
        "# print(l5.shape)\n",
        "# l6=FullyConnected(output_channels=64).forward(l5)\n",
        "# print(l6.shape)\n",
        "# l7=Activation('relu').forward(l6)\n",
        "# print(l7.shape)\n",
        "# l8=FullyConnected(output_channels=10).forward(l7)\n",
        "# print(l8.shape)\n",
        "# l9=Activation('softmax').forward(l8)\n",
        "# print(l9.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "i8Yv10oyGM8U",
        "outputId": "fdde4b3f-ffbc-42ba-a436-c54413169733"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-ab998a823f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cat_cross_entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_layer' is not defined"
          ]
        }
      ],
      "source": [
        "cnn = Model(loss_name='cat_cross_entropy')\n",
        "\n",
        "cnn.add(conv_layer(output_channels=8,kernel_size=3,stride=1,padding=1))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPool(pool_size=2))\n",
        "cnn.add(batch_norm_layer())\n",
        "\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(FullyConnected(output_channels=64))\n",
        "cnn.add(batch_norm_layer())\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(FullyConnected(output_channels=10))\n",
        "cnn.add(batch_norm_layer())\n",
        "cnn.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_9rx3vWnGPFM",
        "outputId": "c7ae7442-5854-4092-cec0-c1c8accb61ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-94b49ba7da5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_c10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-174-0cee61e746c3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epochs, batch_size, learning_rate, x_val, y_val)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# compute loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mthis_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mloss_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mthis_loss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0macc\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-0cee61e746c3>\u001b[0m in \u001b[0;36mcat_cross_entropy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcat_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# def cat_cross_entropy(y_true, y_pred):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (32,32,3) "
          ]
        }
      ],
      "source": [
        "cnn.fit(X_trn_c10,y_trn_c10,10,1e-05,X_val_c10,y_val_c10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sfJxcQWhwqV"
      },
      "outputs": [],
      "source": [
        "def add2d(X, K):  \n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j].assign(tf.reduce_sum(\n",
        "                X[i: i + h, j: j + w] + K))\n",
        "    return Y\n",
        "\n",
        "\n",
        "class adder_2d(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, kernel_size):\n",
        "        initializer = tf.random_normal_initializer()\n",
        "        self.weight = self.add_weight(name='w', shape=kernel_size,\n",
        "                                      initializer=initializer)\n",
        "        self.bias = self.add_weight(name='b', shape=(1, ),\n",
        "                                    initializer=initializer)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return add2d(inputs, self.weight) + self.bias\n",
        "\n",
        "\n",
        "class add_it(tf.keras.layers.Conv2D):\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n",
        "        return tf.nn.conv2d(\n",
        "            inputs,\n",
        "            (kernel - mean) / tf.sqrt(var + 1e-10),\n",
        "            padding=\"VALID\",\n",
        "            strides=list(self.strides),\n",
        "            name=self.__class__.__name__,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BztaNByshHDN"
      },
      "outputs": [],
      "source": [
        "def conv_single_step(window, filter_, bias):\n",
        "    \"\"\"\n",
        "    window -- k_h x k_w x k_d\n",
        "    filter_ -- k_h x k_w x k_d\n",
        "    b      -- 1x1x1\n",
        "    Z      -- scalar\n",
        "    \"\"\"\n",
        "    out = np.sum((np.multiply(window,filter_) + bias.astype(float))).astype(float)\n",
        "    \n",
        "    return out\n",
        "\n",
        "class conv_layer(Layer):\n",
        "    def __init__(self,output_channels,kernel_size=3,stride=1,padding=0):#,similarity_f = L1):\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "\n",
        "        self.output_channels = output_channels\n",
        "        self.adaptive_eta=0\n",
        "\n",
        "        self.kernel_size=kernel_size        \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        filters -- c_out x k_H x k_W x c_in\n",
        "        b       -- c_out x 1 x 1 x 1\n",
        "        Z       -- n_tensors x H_new x W_new, c_out\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        self.input = X\n",
        "\n",
        "        # in case input size not given\n",
        "        self.input_channels = X.shape[-1]\n",
        "\n",
        "        self.filters = np.random.normal(loc=0,scale=1,size=(self.output_channels, self.kernel_size, self.kernel_size, self.input_channels))\n",
        "        self.bias    = np.random.normal(loc=0,scale=1,size=(self.output_channels, 1,1 ,self.input_channels))\n",
        "        \n",
        "        filters,stride,padding,bias = self.filters, self.stride, self.padding, self.bias\n",
        "        n_tensors, H,   W,   c_in = X.shape\n",
        "        c_out,     H_k, W_k, c_in = filters.shape\n",
        "        n_filters = c_out\n",
        "\n",
        "        X_padded = np.pad(X, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        H_new = int((H + 2*padding - H_k)/stride)+1\n",
        "        W_new = int((W + 2*padding - W_k)/stride)+1\n",
        "\n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "\n",
        "        for i in range(n_tensors):           # traverse batch\n",
        "            this_img = X_padded[i,:,:,:]     # select ith image in batch\n",
        "            for f in range(n_filters):       # traverse filters\n",
        "                this_filter = filters[f,:,:,:]\n",
        "                this_bias   = bias[f,:,:,:]\n",
        "                for h in range(H_new):       # traverse height\n",
        "                    for w in range(W_new):   # traverse width\n",
        "                        v0,v1 = h*stride, h*stride + H_k\n",
        "                        h0,h1 = w*stride, w*stride + W_k\n",
        "                        this_window = this_img[v0:v1,h0:h1,:]\n",
        "\n",
        "                        Z[i, h, w, f] = conv_single_step(this_window, this_filter, this_bias) \n",
        "\n",
        "        assert Z.shape == (n_tensors, H_new, W_new, n_filters)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, filters, bias, stride, padding\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "        X, filters, bias, stride, padding = self.cache\n",
        "\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        n_filters, H_k,    W_k,    c_down = filters.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "        \n",
        "        dX       = np.zeros_like(X)                           \n",
        "        dfilters = np.zeros_like(filters)\n",
        "        dbias    = np.zeros_like(bias)\n",
        "\n",
        "        X_padded  = np.pad(X,  ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_padded = np.pad(dX, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_padded[i]\n",
        "            \n",
        "            for h in range(H_up):                   # traverse height\n",
        "                for w in range(W_up):               # traverse width\n",
        "                    for c in range(c_up):           # traverse filters\n",
        "                        \n",
        "                        v0,v1 = h,h+H_k\n",
        "                        h0,h1 = w,w+W_k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:h1, :]\n",
        "                        f_window = filters[c,:,:,:]\n",
        "\n",
        "                        dx_local = hard_tanh(f_window-x_window)\n",
        "                        df_local = x_window-f_window\n",
        "\n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += np.multiply(dx_local,g)\n",
        "                        dfilters[c,:,:,:]   += np.multiply(df_local,g)\n",
        "                        dbias[c,:,:,:]      += g\n",
        "                        \n",
        "            dX[i, :, :, :] = dx[padding:-padding, padding:-padding, :]\n",
        "        \n",
        "        assert(dX.shape == (n_tensors, H_down, W_down, c_down))\n",
        "\n",
        "        self.filters -= learning_rate*dfilters\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0puXBDT4sFDz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
