{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdowner212/cs577_addernet/blob/main/AdderNet_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "xJBCiQ6eoTki"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before running:\n",
        "\n",
        "1. Set `root` (below) to chosen directory"
      ],
      "metadata": {
        "id": "MHklBShmoUGa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e0fhNZSGfHwc"
      },
      "outputs": [],
      "source": [
        "root = os.getcwd() # whatever you want"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Download CIFAR10 data"
      ],
      "metadata": {
        "id": "W3P8qFKGogvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "\n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "data_zip = os.path.join(root,'cifar-10-python.tar.gz')\n",
        "f = tarfile.open(data_zip)\n",
        "f.extractall(root) \n",
        "f.close()\n",
        "os.remove(data_zip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wr8unJnXocpK",
        "outputId": "033c4b8b-95f6-4021-ec9a-e8c523a752a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-16 19:23:08--  https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  18.0MB/s    in 7.6s    \n",
            "\n",
            "2022-11-16 19:23:17 (21.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Q_XpZyGb_QSq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#import tensorflow.keras.backend as K\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "import pickle\n",
        "import sys\n",
        "import tensorflow.keras.utils as np_utils\n",
        "np_config.enable_numpy_behavior()\n",
        "from scipy.sparse import diags\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GcKuPTjCjLEo"
      },
      "outputs": [],
      "source": [
        "# def L1(a,b):\n",
        "#     return np.abs(a-b)\n",
        "\n",
        "def hard_tanh(array):\n",
        "    return np.clip(array,-1,1)\n",
        "\n",
        "def eps():\n",
        "    return np.random.uniform(1e-07,1e-06)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISUhuC_DB9_7"
      },
      "source": [
        "# Layer definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD9vhW-tCBj7"
      },
      "source": [
        "### `Layer` parent class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uH98VMolxy1J"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    def forward(self, X, init_weights):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WkIljknSJGBt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWAo_-8WCFZx"
      },
      "source": [
        "### `Adder` layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {
        "id": "CnPeNX0zDoEQ"
      },
      "outputs": [],
      "source": [
        "# def adder_single_step(window, filter_, similarity_f=L1):\n",
        "#     \"\"\"\n",
        "#     window -- k_h x k_w x k_d\n",
        "#     filter -- k_h x k_w x k_d\n",
        "#     b      -- 1x1x1\n",
        "#     Z      -- scalar\n",
        "#     \"\"\"\n",
        "#     #H_k,W_k,D_k = filter_.shape\n",
        "#     #out=0\n",
        "#     #for h in range(H_k):\n",
        "#     #    for w in range(W_k):\n",
        "#     #        for d in range(D_k):\n",
        "#     #            out += similarity_f(window[h,w,d], filter_[h,w,d])\n",
        "#     return np.abs(window-filter_).sum()\n",
        "#     #return out\n",
        "\n",
        "class adder_layer(Layer):\n",
        "    def __init__(self,output_channels,kernel_size=3,stride=1,padding=0,adaptive_eta=0):\n",
        "        self.output_channels = output_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.adaptive_eta=adaptive_eta\n",
        "        self.kernel_size=kernel_size        \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.filters = np.ones((self.output_channels,self.kernel_size,self.kernel_size,1))\n",
        "        self.bias = np.zeros((self.output_channels,1,1,1))\n",
        "\n",
        "    def get_adaptive_lr(self, k, dfilters, eta):\n",
        "        \"\"\"    \n",
        "        k           -- n_tensors \n",
        "        dfilters    -- c_out x k_H x k_W x c_in\n",
        "        eta         -- scalar\n",
        "        \"\"\"\n",
        "        norm = np.linalg.norm(dfilters, ord=2, axis=0)\n",
        "        return (eta * np.sqrt(k)) / (norm+eps())\n",
        "    def forward(self,X, init_weights=False):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        filters -- c_out x k_H x k_W x c_in\n",
        "        b       -- c_out x 1 x 1 x 1\n",
        "        Z       -- n_tensors x H_new x W_new, c_out\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        self.input = X\n",
        "        self.input_channels = X.shape[-1]\n",
        "        if init_weights==True:\n",
        "            self.filters = np.random.normal(loc=0,scale=1,size=(self.output_channels, self.kernel_size, self.kernel_size, self.input_channels))\n",
        "            self.bias = np.zeros((self.output_channels,1,1,self.input_channels))\n",
        "\n",
        "        filters, bias, stride,padding = self.filters, self.bias, self.stride, self.padding\n",
        "\n",
        "        # single images:\n",
        "        # H, W, c_in = X.shape\n",
        "        # batches:\n",
        "        n_tensors, H,   W,   c_in = X.shape\n",
        "        c_out,     H_k, W_k, c_in = filters.shape\n",
        "        n_filters = c_out\n",
        "\n",
        "\n",
        "        # single images:\n",
        "        # X_padded = np.pad(X, ((padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        # batches:\n",
        "        X_padded = np.pad(X, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        H_new = int((H + 2*padding - H_k)/stride)+1\n",
        "        W_new = int((W + 2*padding - W_k)/stride)+1\n",
        "\n",
        "\n",
        "        # batches: \n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "        # single images:\n",
        "        # Z = np.zeros([H_new,W_new,c_out])\n",
        "\n",
        "        # batches:\n",
        "        for i in range(n_tensors):           # traverse batch\n",
        "            # batches: \n",
        "            this_img = X_padded[i,:,:,:]     # select ith image in batch\n",
        "            for f in range(n_filters):       # traverse filters\n",
        "                this_filter = filters[f,:,:,:]\n",
        "                this_bias = bias[f,:,:,:]\n",
        "                for h in range(H_new-H_k):   # traverse height\n",
        "                    for w in range(W_new):   # traverse width\n",
        "                        v0,v1 = h*stride, h*stride + H_k\n",
        "                        h0,h1 = w*stride, w*stride + W_k\n",
        "                        this_window = this_img[v0:v1,h0:h1,:]\n",
        "\n",
        "                        # batches:\n",
        "                        Z[i, h, w, f] = np.abs(this_window-this_filter).sum()\n",
        "                        # single images\n",
        "                        # Z[h,w,f] = np.abs(this_window-this_filter).sum() \n",
        "\n",
        "        # batches:\n",
        "        assert Z.shape == (n_tensors, H_new, W_new, n_filters)\n",
        "        # single images:\n",
        "        # assert Z.shape == (H_new, W_new, n_filters)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, filters, bias, stride, padding\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "        \n",
        "        X, filters, bias, stride, padding = self.cache\n",
        "\n",
        "        # single images:\n",
        "        #H_down, W_down, c_down = X.shape\n",
        "        # batches:\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "\n",
        "        n_filters, H_k,    W_k,    c_down = filters.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "        \n",
        "        dX       = np.zeros_like(X)                           \n",
        "        dfilters = np.zeros_like(filters)\n",
        "        dbias    = np.zeros_like(bias)\n",
        "\n",
        "\n",
        "        # batches:\n",
        "        X_padded  = np.pad(X,  ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_padded = np.pad(dX, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        # single images:\n",
        "        # X_padded  = np.pad(X,  ((padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        # dX_padded = np.pad(dX, ((padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "\n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_padded[i]\n",
        "            \n",
        "        #x, dx = X_padded, dX_padded\n",
        "            for h in range(H_up):                   # traverse height\n",
        "                for w in range(W_up):               # traverse width\n",
        "                    for c in range(c_up):           # traverse filters\n",
        "                        \n",
        "                        v0,v1 = h,h+H_k\n",
        "                        h0,h1 = w,w+W_k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:h1, :]\n",
        "                        f_window = filters[c,:,:,:]\n",
        "\n",
        "                        dx_local = hard_tanh(f_window-x_window)\n",
        "                        df_local = x_window-f_window\n",
        "\n",
        "                        # single images:\n",
        "                        # g = upstream_g[h,w,c]\n",
        "                        # batches:\n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += dx_local * g\n",
        "                        dfilters[c,:,:,:]   += df_local * g\n",
        "                        dbias[c,:,:,:]      += g\n",
        "\n",
        "            # single images:        \n",
        "            # dX = dx[padding:-padding, padding:-padding,:]\n",
        "            # batches:\n",
        "            dX[i, :, :, :] = dx[padding:-padding, padding:-padding, :]\n",
        "\n",
        "        #print('adder_layer:')\n",
        "        #print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "        #print(f'dF min, max = {str(round(np.min(dfilters.flatten()),3))},{str(round(np.max(dfilters.flatten()),3))}')\n",
        "\n",
        "        # single images:\n",
        "        # assert (dX.shape == (H_down, W_down, c_down))\n",
        "        # batches:\n",
        "        assert(dX.shape == (n_tensors, H_down, W_down, c_down))\n",
        "\n",
        "\n",
        "        adaptive_lr = self.get_adaptive_lr(n_filters, dfilters, self.adaptive_eta)\n",
        "\n",
        "        self.filters -= learning_rate*adaptive_lr*dfilters\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "        #plt.plot(range(len(dfilters.flatten())),dfilters.flatten())\n",
        "        #plt.title('adder_layer.backward: dfilters values')\n",
        "        #plt.show()\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVevHrit5823"
      },
      "source": [
        "### Fully-connected layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "id": "Ut73yN6458Nc"
      },
      "outputs": [],
      "source": [
        "class FullyConnected(Layer):\n",
        "    def __init__(self,output_channels):\n",
        "        super(Layer, self).__init__()\n",
        "        self.output_channels = output_channels\n",
        "        self.weights=np.ones((1,self.output_channels))\n",
        "        self.bias=np.zeros((1,self.output_channels))\n",
        "\n",
        "    def forward(self, X,init_weights):\n",
        "        self.input = X\n",
        "        self.input_channels = X.shape[-1]\n",
        "        if init_weights==True:\n",
        "            self.weights = np.random.normal(loc=0,scale=1,size=(self.input_channels,self.output_channels))\n",
        "            self.bias    = np.random.normal(loc=0,scale=1,size=(self.output_channels))\n",
        "\n",
        "        self.output = np.dot(self.input, self.weights)\n",
        "        for i in range(self.output.shape[0]):\n",
        "            self.output[i] += self.bias\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        dX    = np.dot(upstream_g, self.weights.T)\n",
        "        dW    = np.dot(self.input.T, upstream_g)\n",
        "        dbias = np.mean(upstream_g)\n",
        "        self.weights -= learning_rate*dW\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "\n",
        "        #print('FullyConnected:')\n",
        "        #print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "        #print(f'dW min, max = {str(round(np.min(dW.flatten()),3))},{str(round(np.max(dW.flatten()),3))}')\n",
        "\n",
        "\n",
        "        #plt.plot(range(len(dW.flatten())),dW.flatten())\n",
        "        #plt.title('FullyConnected.backward: dW values')\n",
        "        #plt.show()\n",
        "        return dX\n",
        "\n",
        "'''\n",
        "     25 \n",
        "     26         self.weights -= learning_rate*dW\n",
        "---> 27         self.bias    -= learning_rate*dbias\n",
        "     28 \n",
        "     29         print('FullyConnected:')\n",
        "\n",
        "ValueError: non-broadcastable output operand with shape (10,) doesn't match the broadcast shape (16,10)\n",
        "''';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuAVweO_Apj4"
      },
      "source": [
        "### Flatten layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "J_v6tBkaArdE"
      },
      "outputs": [],
      "source": [
        "class Flatten(Layer):\n",
        "    def forward(self, X,init_weights):\n",
        "        self.original_shape = X.shape\n",
        "        # single images:\n",
        "        # self.output = X\n",
        "        # batches:\n",
        "        self.output = X.reshape(X.shape[0], np.product(X.shape[1:]))\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        dX = upstream_g.reshape(self.original_shape)\n",
        "        #print('Flatten:')\n",
        "        #print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjpocIPJHz-0"
      },
      "source": [
        "### BatchNorm Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 495,
      "metadata": {
        "id": "kpzQWoTFH1rv"
      },
      "outputs": [],
      "source": [
        "class batch_norm_layer(Layer):\n",
        "    def __init__(self, gamma=None,beta=None):\n",
        "\n",
        "        gamma = None if not gamma else gamma\n",
        "        beta = None if not beta else beta\n",
        "        self.gamma = 1 #np.ones((1,1,1,1))\n",
        "        self.beta = 0  #np.zeros((1,1,1,1))\n",
        "\n",
        "\n",
        "    def forward(self, X,init_weights):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        gamma   -- n_tensors x 1 x 1 x 1\n",
        "        beta    -- n_tensors x 1 x 1 x 1\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "\n",
        "        self.input = X\n",
        "\n",
        "        if init_weights==True:\n",
        "            # single images:\n",
        "            # self.gamma = np.ones((1,1,1))\n",
        "            # self.beta = np.zeros((1,1,1))\n",
        "            # batch:\n",
        "            self.gamma = np.ones((1,1,1))\n",
        "            self.beta = np.zeros((1,1,1))\n",
        "\n",
        "        mean = np.mean(X,axis=(0, 1, 2), keepdims=True)\n",
        "        var = np.mean(((X-mean)**2), axis=(0, 1, 2), keepdims=True)\n",
        "        std = np.sqrt(var)\n",
        "        \n",
        "        X_center = X - mean\n",
        "        X_norm = X_center/(std+eps())\n",
        "\n",
        "        #out = X_norm*self.gamma\n",
        "        \n",
        "        self.output = X_norm*self.gamma + self.beta\n",
        "        self.cache = X, X_center, X_norm\n",
        "\n",
        "        return self.output \n",
        "        '''\n",
        "        ---> 36         self.output = X_norm*self.gamma + self.beta\n",
        "            37         self.cache = X, X_center, X_norm\n",
        "            38 \n",
        "\n",
        "        ValueError: operands could not be broadcast together with shapes (33,16,16,8) (3,16,16,8) \n",
        "        '''\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, X_norm)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "\n",
        "        X, X_center, X_norm = self.cache\n",
        "\n",
        "        dGamma = np.sum(upstream_g * X_norm, axis=0)\n",
        "        dBeta  = np.sum(upstream_g, axis=0)\n",
        "\n",
        "        m = len(X)\n",
        "        mean = np.mean(X)\n",
        "        std = np.std(X)\n",
        "        \n",
        "        dX = np.zeros_like(X)\n",
        "\n",
        "        for i in range(m):\n",
        "            for j in range(m):\n",
        "                dX[i] += (upstream_g[i] - upstream_g[j]*(1 + (X[i]-X[j])*(X[j]-mean)/std))\n",
        "\n",
        "        dX *= self.gamma/((m**2)*std)\n",
        "        \n",
        "        self.gamma = self.gamma - learning_rate*dGamma\n",
        "        self.beta  = self.beta  - learning_rate*dBeta\n",
        "\n",
        "\n",
        "       # print('batch_norm_layer:')\n",
        "       # print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2t7dqw4ngiD"
      },
      "source": [
        "### Maxpool layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 496,
      "metadata": {
        "id": "_zJEQA1fAzVS"
      },
      "outputs": [],
      "source": [
        "class MaxPool(Layer):\n",
        "    def __init__(self,pool_size=2):\n",
        "        self.pool_size=pool_size\n",
        "        self.stride = pool_size\n",
        "\n",
        "    def forward(self,X,init_weights):\n",
        "        # single images:\n",
        "        # H,W,c_in = X.shape \n",
        "        # batches: \n",
        "        n_tensors, H, W, c_in = X.shape\n",
        "\n",
        "\n",
        "\n",
        "        H_new = int(1 + (H - self.pool_size) / self.stride)\n",
        "        W_new = int(1 + (W - self.pool_size) / self.stride)\n",
        "        c_out = c_in\n",
        "        \n",
        "        # single images:\n",
        "        # Z = np.zeros((H_new,W_new,c_out))\n",
        "        # batches: \n",
        "        Z = np.zeros((n_tensors, H_new, W_new, c_out))              \n",
        "        \n",
        "        for i in range(n_tensors):                     # loop over the training examples\n",
        "            for h in range(H_new):                     # loop on the vertical axis of the output volume\n",
        "                for w in range(W_new):                 # loop on the horizontal axis of the output volume\n",
        "                    for c in range(c_out):             # loop over the channels of the output volume\n",
        "                        \n",
        "                        v0,v1 = h*self.stride, h*self.stride + self.pool_size\n",
        "                        h0,h1 = w*self.stride, w*self.stride + self.pool_size\n",
        "                        \n",
        "                        # single images:\n",
        "                        # window = X[v0:v1, h0:h1, c]\n",
        "                        # Z[h,w,c] = np.max(window)\n",
        "                        # batches:\n",
        "                        window = X[i, v0:v1, h0:h1,c]\n",
        "                        Z[i, h, w, c] = np.max(window)\n",
        "                    \n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, self.pool_size, self.stride\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g,learning_rate):\n",
        "        X, pool_size, stride = self.cache\n",
        "\n",
        "        # batches: \n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        # single images:\n",
        "        # H_down, W_down, c_down = X.shape \n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "\n",
        "\n",
        "        dX = np.zeros(X.shape)\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X[i]\n",
        "            for h in range(H_up):       \n",
        "                for w in range(W_up):    \n",
        "                    for c in range(c_up):       \n",
        "                        v0,v1 = h, h+pool_size\n",
        "                        h0,h1 = w, w+pool_size\n",
        "\n",
        "                        x_window = x[v0:v1, h0:h1, c]\n",
        "                        \n",
        "                        local_g = np.where(x_window==np.max(x_window),1,0)\n",
        "                        # single images:\n",
        "                        # g = upstream_g[h,w,c] \n",
        "                        # batches: \n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        # single images:  \n",
        "                        # dX[v0:v1, h0:h1, c] += local_g*g  \n",
        "                        # batches: \n",
        "                        dX[i, v0:v1, h0:h1, c] += local_g * g\n",
        "\n",
        "        assert(dX.shape == X.shape)\n",
        "     #   print('MaxPool:')\n",
        "     #   print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "\n",
        "  \n",
        "        return dX\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO7MWk0_B3XO"
      },
      "source": [
        "### Activation layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 497,
      "metadata": {
        "id": "jLa7JtufB6Tr"
      },
      "outputs": [],
      "source": [
        "def relu_fwd(X):\n",
        "    return np.where(X>=0,X,0)\n",
        "def relu_bwd(X):\n",
        "    return np.where(X>=0,1,0)\n",
        "\n",
        "\n",
        "def softmax_fwd(x):\n",
        "    soft = tf.nn.softmax(x)\n",
        "    return soft.numpy()\n",
        "\n",
        "def softmax_bwd(X): \n",
        "    s = softmax_fwd(X)\n",
        "    J = np.zeros_like(s)\n",
        "\n",
        "    for i in range(len(s)):\n",
        "        for j in range(len(s[i])):\n",
        "            indicator_ij = 1 if i==j else 0\n",
        "            J[i][j] = s[i][j]*(indicator_ij - s[i][j])\n",
        "    return J\n",
        "\n",
        "\n",
        "\n",
        "activation_dict = {'relu':    {'forward':  relu_fwd,\n",
        "                               'backward': relu_bwd},\n",
        "                   'softmax': {'forward':  softmax_fwd,\n",
        "                               'backward': softmax_bwd}}\n",
        "\n",
        "class Activation(Layer):\n",
        "    def __init__(self,activation_name):\n",
        "        self.activation_name = activation_name\n",
        "        super(Layer, self).__init__()\n",
        "    def forward(self, X, init_weights=False):\n",
        "        self.input = X\n",
        "        if self.activation_name == 'relu':\n",
        "            self.output = np.where(X>=0,X,0)\n",
        "            return self.output\n",
        "        elif self.activation_name == 'softmax':\n",
        "            self.output = tf.nn.softmax(X).numpy()\n",
        "            return self.output\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "\n",
        "        local_g = None     \n",
        "        if self.activation_name == 'relu':\n",
        "            local_g = np.where(self.input>=0,1,0)\n",
        "\n",
        "        elif self.activation_name == 'softmax':\n",
        "            s = self.output\n",
        "            J = np.zeros_like(s)\n",
        "            for i in range(len(s)):\n",
        "                for j in range(len(s[i])):\n",
        "                    indicator_ij = 1 if i==j else 0\n",
        "                    J[i][j] = s[i][j]*(indicator_ij - s[i][j])\n",
        "            local_g = J\n",
        "\n",
        "        #print('local_g:',local_g)\n",
        "        #print('upstream_g:',upstream_g)\n",
        "        dX = np.zeros_like(self.input)\n",
        "        for i, (l,u) in enumerate(zip(local_g, upstream_g)):\n",
        "            dX[i]=learning_rate*l*u\n",
        "        \n",
        "        #learning_rate*np.dot(local_g,upstream_g.T)\n",
        "  \n",
        "        # print(f'Activation ({self.activation_name}):')\n",
        "        # print(f'local_g min, max = {str(round(np.min(local_g.flatten()),3))},{str(round(np.max(local_g.flatten()),3))}')\n",
        "        # print(f'up_g min, max = {str(round(np.min(upstream_g.flatten()),3))},{str(round(np.max(upstream_g.flatten()),3))}')\n",
        "        # print(f'dX min, max = {str(round(np.min(dX.flatten()),3))},{str(round(np.max(dX.flatten()),3))}')\n",
        "        return dX\n",
        "\n",
        "        #print(f'{self.activation_name} activation layer:  min dX = {str(round(np.min(out.flatten()),3))}, max dX = {str(round(np.max(out.flatten()),3))}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKx4NGICC-74"
      },
      "source": [
        "### `Model` class"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l33xDKCtZzf4"
      },
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {
        "id": "iaEag2IkC-JR"
      },
      "outputs": [],
      "source": [
        "def cat_cross_entropy(y_true, y_pred):    \n",
        "    out = -np.sum(np.multiply(y_true,np.log(y_pred+eps())))\n",
        "    return out/y_true.shape[0]\n",
        "\n",
        "# def cat_cross_entropy(y_true, y_pred):    \n",
        "#     return tf.keras.losses.CategoricalCrossentropy()(y_true,y_pred).numpy()\n",
        "\n",
        "def cat_cross_entropy_prime(y_true,y_pred):\n",
        "    return np.sum([-y/(yhat+eps()) for (y,yhat) in zip(y_true,y_pred)])\n",
        "\n",
        "\n",
        "loss_dict = {'cat_cross_entropy':    {'forward':  cat_cross_entropy,\n",
        "                                      'backward': cat_cross_entropy_prime}}\n",
        "\n",
        "def get_mini_batches(X,y,batch_size):\n",
        "    mini_batches = []\n",
        "    for i in range(0,len(X), batch_size):\n",
        "        lower = i\n",
        "        upper = np.min([len(X), i + batch_size])\n",
        "        X_batch = X[lower:upper]\n",
        "        y_batch = y[lower:upper]\n",
        "        mini_batches.append((X_batch,y_batch))\n",
        "\n",
        "    return mini_batches\n",
        "\n",
        "class Model:\n",
        "    def __init__(self,loss_name): \n",
        "        self.layers = []\n",
        "        self.loss_fwd = loss_dict[loss_name]['forward']\n",
        "\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        y_hat = []\n",
        "        Z = input_data\n",
        "        for layer in self.layers:\n",
        "            Z = layer.forward(Z,init_weights=False)\n",
        "        y_hat = Z\n",
        "        return y_hat\n",
        "\n",
        "    def fit(self, x_train, y_train, epochs, batch_size, learning_rate, x_val=None, y_val=None):\n",
        "        history = {'accuracy': [],'loss': [],'val_accuracy': [],'val_loss': []}\n",
        "\n",
        "        for e in range(epochs):\n",
        "            print(e)\n",
        "\n",
        "            loss_,acc,val_loss,val_acc=0,0,0,0\n",
        "\n",
        "            mini_batches = get_mini_batches(x_train,y_train, batch_size)\n",
        "\n",
        "            # single images:\n",
        "            # for i,img in enumerate(x_train):\n",
        "            # batches: \n",
        "            for i, mini_batch in enumerate(mini_batches):    \n",
        "                print(i)\n",
        "\n",
        "                # single images:\n",
        "                # x_batch= img\n",
        "                # y_batch= y_train[i] \n",
        "                # batches: \n",
        "                x_batch = mini_batch[0]\n",
        "                y_batch = mini_batch[1]\n",
        "\n",
        "                # forward\n",
        "                Z = x_batch\n",
        "\n",
        "                for layer in self.layers:\n",
        "                    #print(layer)\n",
        "                    init_weights=True if e==0 else False\n",
        "                    Z = layer.forward(Z,init_weights=init_weights)\n",
        "                y_real = y_batch\n",
        "                y = np.argmax(y_real,axis=1)\n",
        "                y_pred = Z\n",
        "\n",
        "                # compute loss and accuracy\n",
        "                # loss defined in line:\n",
        "                m = len(y)\n",
        "                #this_loss = -np.log(Z[range(m),y])\n",
        "                #loss_ += this_loss\n",
        "                #-np.sum(y_true * np.log(y_pred + 10**-100))\n",
        "                # if calling loss function explicitly:\n",
        "                this_loss = self.loss_fwd(y_real, y_pred)\n",
        "\n",
        "                acc   += sum(np.where(np.argmax(y_real,axis=1)==np.argmax(y_pred,axis=1),1,0))\n",
        "                print(f'batch: {i+1}/{len(mini_batches)+1}'.ljust(15) + f'this batch loss: {str(round(np.sum(this_loss),3))}','\\r')#, flush=True)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "                # backward - dCCE/dsoftmax\n",
        "                # Z[:,y] -= 1\n",
        "                # Z /= len(Z)\n",
        "                # error = Z\n",
        "                error = -y/(np.argmax(y_pred,axis=1) + eps())\n",
        "\n",
        "                for layer in (self.layers)[::-1]:\n",
        "                    #print(layer)\n",
        "                    error = layer.backward(error, learning_rate)\n",
        "              \n",
        "            loss_ /= x_train.shape[0]\n",
        "            acc  /= x_train.shape[0]\n",
        "            \n",
        "            history['loss'].append(loss_)\n",
        "            history['accuracy'].append(acc)\n",
        "\n",
        "            if x_val is None or y_val is None:\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss_,3))}   acc = {str(round(acc,3))}')\n",
        "            else:\n",
        "                ## single images:\n",
        "                # val_acc=0\n",
        "                # for i,img in enumerate(x_val):\n",
        "                #     Z_val = img\n",
        "                #     for layer in self.layers:\n",
        "                #         Z_val = layer.forward(Z_val, init_weights=False)\n",
        "\n",
        "                #     y_real_val = y_val[i]\n",
        "                #     y_pred_val= Z_val\n",
        "                #     val_loss = self.loss_fwd(y_real_val, y_pred_val)\n",
        "                #     val_acc += 1 if np.argmax(y_real_val,axis=1)==np.argmax(y_pred_val,axis=1) else 0\n",
        "\n",
        "\n",
        "                ## batches:\n",
        "                Z_val = x_val\n",
        "                for layer in self.layers:\n",
        "                   Z_val = layer.forward(Z_val,init_weights=False)\n",
        "\n",
        "                y_real_val = y_val\n",
        "                y_pred_val = Z_val\n",
        "\n",
        "                val_loss = self.loss_fwd(y_real_val, y_pred_val)\n",
        "                val_acc = sum(np.where(np.argmax(y_real_val,axis=1)==np.argmax(y_pred_val,axis=1),1,0))\n",
        "                val_acc  /= x_val.shape[0]                \n",
        "\n",
        "                history['val_accuracy'].append(val_acc)\n",
        "                history['val_loss'].append(val_loss)\n",
        "\n",
        "                print(f'Epoch: {e}   loss = {str(round(loss_,3))}   acc = {str(round(acc,3))}   val_loss = {str(round(val_loss,3))}   val_accuracy = {str(round(val_acc,3))}')\n",
        "\n",
        "        return history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbHmgJRtCj69"
      },
      "source": [
        "## Data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Oo5KOmoCzNm"
      },
      "source": [
        "### CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {
        "id": "uHl1tum3l-oL"
      },
      "outputs": [],
      "source": [
        "def load_cifar_data(folder,tiny=False):\n",
        "    train_batches = [f'{folder}/{f}' for f in os.listdir(folder) if 'batch_' in f]\n",
        "    test_batch    =  f'{folder}/test_batch'\n",
        "\n",
        "    # Get train data\n",
        "    X_trn = None\n",
        "    y_trn = []\n",
        "    for i in range(len(train_batches)):\n",
        "        train_data_dict = pickle.load(open(train_batches[i],'rb'), encoding='latin-1')\n",
        "        if i+1 == 1:\n",
        "            X_trn = train_data_dict['data']\n",
        "        else:\n",
        "            X_trn = np.vstack((X_trn, train_data_dict['data']))\n",
        "        y_trn += train_data_dict['labels']\n",
        "    X_trn = X_trn.reshape(len(X_trn),3,32,32)\n",
        "    X_trn = np.rollaxis(X_trn,1,4)\n",
        "    X_trn = X_trn.astype('float32')/255.0\n",
        "    y_trn = np_utils.to_categorical(np.asarray(y_trn),10)\n",
        "\n",
        "    # Get test data\n",
        "    test_data_dict  = pickle.load(open(test_batch,'rb'), encoding='latin-1')\n",
        "    X_tst = test_data_dict['data']\n",
        "    X_tst = X_tst.reshape(len(X_tst),3,32,32)\n",
        "    X_tst = np.rollaxis(X_tst,1,4)\n",
        "    X_tst = X_tst.astype('float32')/255.0\n",
        "    y_tst = np_utils.to_categorical(np.asarray(test_data_dict['labels']))\n",
        "    \n",
        "    n_90 = int(0.9*len(X_trn))\n",
        "    X_trn, X_val = X_trn[:n_90], X_trn[n_90:]\n",
        "    y_trn, y_val = y_trn[:n_90], y_trn[n_90:]\n",
        "\n",
        "    if tiny:\n",
        "        X_trn,y_trn,X_tst,y_tst,X_val,y_val = X_trn[:500],y_trn[:500],X_tst[:100],y_tst[:100],X_val[:50],y_val[:50]\n",
        "\n",
        "    return X_trn, y_trn, X_tst, y_tst, X_val, y_val\n",
        "\n",
        "data_dir = f'{root}/cifar-10-batches-py'\n",
        "X_trn_c10, y_trn_c10, X_tst_c10, y_tst_c10, X_val_c10, y_val_c10 = load_cifar_data(data_dir,tiny=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {
        "id": "kRIJNY0dRvD2"
      },
      "outputs": [],
      "source": [
        "this_model = Model(loss_name='cat_cross_entropy')\n",
        "\n",
        "this_model.add(adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1,adaptive_eta=0.1))\n",
        "this_model.add(Activation('relu'))\n",
        "this_model.add(MaxPool(pool_size=2))\n",
        "this_model.add(batch_norm_layer())\n",
        "\n",
        "\n",
        "this_model.add(Flatten())\n",
        "this_model.add(FullyConnected(output_channels=64))\n",
        "this_model.add(Activation('relu'))\n",
        "this_model.add(FullyConnected(output_channels=10))\n",
        "this_model.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(np.array([-6.25000000e-02,  4.72255243e-09, -6.25000000e-02, -6.25000000e-02,\n",
        "  -6.25000000e-02, -6.25000000e-02, -4.72255243e-09, -6.25000000e-02,\n",
        "  -6.25000000e-02, -6.25000000e-02]))"
      ],
      "metadata": {
        "id": "2KMcVeopvMol",
        "outputId": "0f6f206c-0531-464f-d73d-091ba57695d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.49999999999999994"
            ]
          },
          "metadata": {},
          "execution_count": 501
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBl1GKVe9MCg",
        "outputId": "4f614581-c567-4985-a106-bb05a04a9bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n",
            "batch: 1/33    this batch loss: 15.88 \n",
            "1\n",
            "batch: 2/33    this batch loss: 14.019 \n",
            "2\n",
            "batch: 3/33    this batch loss: 13.86 \n",
            "3\n",
            "batch: 4/33    this batch loss: 15.367 \n",
            "4\n",
            "batch: 5/33    this batch loss: 11.162 \n",
            "5\n",
            "batch: 6/33    this batch loss: 13.415 \n",
            "6\n",
            "batch: 7/33    this batch loss: 12.266 \n",
            "7\n",
            "batch: 8/33    this batch loss: 12.971 \n",
            "8\n",
            "batch: 9/33    this batch loss: 10.431 \n",
            "9\n",
            "batch: 10/33   this batch loss: 11.743 \n",
            "10\n",
            "batch: 11/33   this batch loss: 13.013 \n",
            "11\n",
            "batch: 12/33   this batch loss: 11.33 \n",
            "12\n",
            "batch: 13/33   this batch loss: 9.628 \n",
            "13\n",
            "batch: 14/33   this batch loss: 15.634 \n",
            "14\n",
            "batch: 15/33   this batch loss: 11.482 \n",
            "15\n",
            "batch: 16/33   this batch loss: 14.619 \n",
            "16\n",
            "batch: 17/33   this batch loss: 13.125 \n",
            "17\n",
            "batch: 18/33   this batch loss: 11.016 \n",
            "18\n",
            "batch: 19/33   this batch loss: 11.48 \n",
            "19\n",
            "batch: 20/33   this batch loss: 14.345 \n",
            "20\n",
            "batch: 21/33   this batch loss: 14.421 \n",
            "21\n",
            "batch: 22/33   this batch loss: 12.188 \n",
            "22\n",
            "batch: 23/33   this batch loss: 11.648 \n",
            "23\n",
            "batch: 24/33   this batch loss: 11.835 \n",
            "24\n",
            "batch: 25/33   this batch loss: 13.238 \n",
            "25\n",
            "batch: 26/33   this batch loss: 12.418 \n",
            "26\n",
            "batch: 27/33   this batch loss: 11.761 \n",
            "27\n",
            "batch: 28/33   this batch loss: 12.155 \n"
          ]
        }
      ],
      "source": [
        "history = this_model.fit(X_trn_c10,y_trn_c10,epochs=20,batch_size=16,learning_rate=1e-05,x_val=X_val_c10,y_val=y_val_c10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(20),history['accuracy'],label='train')\n",
        "plt.plot(range(20),history['val_accuracy'],label='val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.xticks(range(0,21,2))\n",
        "plt.ylim(0,0.3)\n",
        "plt.title('Accuracy -- our AdderNet implementation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XI4_IdBIqhNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(20),history['loss'],label='train')\n",
        "plt.plot(range(20),history['val_loss'],label='val')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.xticks(range(0,21,2))\n",
        "plt.ylim(0,0.1)\n",
        "plt.title('Loss -- our AdderNet implementation')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "2CDP87ePA6ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this_model = Model(loss_name='cat_cross_entropy')\n",
        "# l1 = X_trn_c10\n",
        "# l1=adder_layer(output_channels=8,kernel_size=3,stride=1,padding=1).forward(l1)\n",
        "# print(l1.shape)\n",
        "# l2=Activation('relu').forward(l1)\n",
        "# print(l2.shape)\n",
        "# l3=MaxPool(pool_size=2).forward(l2)\n",
        "# print(l3.shape)\n",
        "# l4=batch_norm_layer().forward(l3)\n",
        "# print(l4.shape)\n",
        "\n",
        "# l5=Flatten().forward(l4)\n",
        "# print(l5.shape)\n",
        "# l6=FullyConnected(output_channels=64).forward(l5)\n",
        "# print(l6.shape)\n",
        "# l7=Activation('relu').forward(l6)\n",
        "# print(l7.shape)\n",
        "# l8=FullyConnected(output_channels=10).forward(l7)\n",
        "# print(l8.shape)\n",
        "# l9=Activation('softmax').forward(l8)\n",
        "# print(l9.shape)"
      ],
      "metadata": {
        "id": "W8_z0zzAqNIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "i8Yv10oyGM8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "outputId": "fdde4b3f-ffbc-42ba-a436-c54413169733"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-ab998a823f6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cat_cross_entropy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'conv_layer' is not defined"
          ]
        }
      ],
      "source": [
        "cnn = Model(loss_name='cat_cross_entropy')\n",
        "\n",
        "cnn.add(conv_layer(output_channels=8,kernel_size=3,stride=1,padding=1))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPool(pool_size=2))\n",
        "cnn.add(batch_norm_layer())\n",
        "\n",
        "\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(FullyConnected(output_channels=64))\n",
        "cnn.add(batch_norm_layer())\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(FullyConnected(output_channels=10))\n",
        "cnn.add(batch_norm_layer())\n",
        "cnn.add(Activation('softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "_9rx3vWnGPFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "c7ae7442-5854-4092-cec0-c1c8accb61ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-180-94b49ba7da5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_trn_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_val_c10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val_c10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-174-0cee61e746c3>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train, epochs, batch_size, learning_rate, x_val, y_val)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# compute loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mthis_loss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mloss_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mthis_loss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0macc\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-174-0cee61e746c3>\u001b[0m in \u001b[0;36mcat_cross_entropy\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcat_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# def cat_cross_entropy(y_true, y_pred):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (32,32,3) "
          ]
        }
      ],
      "source": [
        "cnn.fit(X_trn_c10,y_trn_c10,10,1e-05,X_val_c10,y_val_c10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "-sfJxcQWhwqV"
      },
      "outputs": [],
      "source": [
        "def add2d(X, K):  \n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = tf.Variable(tf.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j].assign(tf.reduce_sum(\n",
        "                X[i: i + h, j: j + w] + K))\n",
        "    return Y\n",
        "\n",
        "\n",
        "class adder_2d(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def build(self, kernel_size):\n",
        "        initializer = tf.random_normal_initializer()\n",
        "        self.weight = self.add_weight(name='w', shape=kernel_size,\n",
        "                                      initializer=initializer)\n",
        "        self.bias = self.add_weight(name='b', shape=(1, ),\n",
        "                                    initializer=initializer)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return add2d(inputs, self.weight) + self.bias\n",
        "\n",
        "\n",
        "class add_it(tf.keras.layers.Conv2D):\n",
        "    def convolution_op(self, inputs, kernel):\n",
        "        mean, var = tf.nn.moments(kernel, axes=[0, 1, 2], keepdims=True)\n",
        "        return tf.nn.conv2d(\n",
        "            inputs,\n",
        "            (kernel - mean) / tf.sqrt(var + 1e-10),\n",
        "            padding=\"VALID\",\n",
        "            strides=list(self.strides),\n",
        "            name=self.__class__.__name__,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "BztaNByshHDN"
      },
      "outputs": [],
      "source": [
        "def conv_single_step(window, filter_, bias):\n",
        "    \"\"\"\n",
        "    window -- k_h x k_w x k_d\n",
        "    filter_ -- k_h x k_w x k_d\n",
        "    b      -- 1x1x1\n",
        "    Z      -- scalar\n",
        "    \"\"\"\n",
        "    out = np.sum((np.multiply(window,filter_) + bias.astype(float))).astype(float)\n",
        "    \n",
        "    return out\n",
        "\n",
        "class conv_layer(Layer):\n",
        "    def __init__(self,output_channels,kernel_size=3,stride=1,padding=0):#,similarity_f = L1):\n",
        "        self.output_channels = output_channels\n",
        "\n",
        "\n",
        "        self.output_channels = output_channels\n",
        "        self.adaptive_eta=0\n",
        "\n",
        "        self.kernel_size=kernel_size        \n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,X):\n",
        "        \"\"\"    \n",
        "        X       -- n_tensors x H x W x c_in\n",
        "        filters -- c_out x k_H x k_W x c_in\n",
        "        b       -- c_out x 1 x 1 x 1\n",
        "        Z       -- n_tensors x H_new x W_new, c_out\n",
        "        cache   -- info needed for backward pass\n",
        "        \"\"\"\n",
        "        self.input = X\n",
        "\n",
        "        # in case input size not given\n",
        "        self.input_channels = X.shape[-1]\n",
        "\n",
        "        self.filters = np.random.normal(loc=0,scale=1,size=(self.output_channels, self.kernel_size, self.kernel_size, self.input_channels))\n",
        "        self.bias    = np.random.normal(loc=0,scale=1,size=(self.output_channels, 1,1 ,self.input_channels))\n",
        "        \n",
        "        filters,stride,padding,bias = self.filters, self.stride, self.padding, self.bias\n",
        "        n_tensors, H,   W,   c_in = X.shape\n",
        "        c_out,     H_k, W_k, c_in = filters.shape\n",
        "        n_filters = c_out\n",
        "\n",
        "        X_padded = np.pad(X, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        H_new = int((H + 2*padding - H_k)/stride)+1\n",
        "        W_new = int((W + 2*padding - W_k)/stride)+1\n",
        "\n",
        "        Z = np.zeros([n_tensors, H_new, W_new, c_out])\n",
        "\n",
        "        for i in range(n_tensors):           # traverse batch\n",
        "            this_img = X_padded[i,:,:,:]     # select ith image in batch\n",
        "            for f in range(n_filters):       # traverse filters\n",
        "                this_filter = filters[f,:,:,:]\n",
        "                this_bias   = bias[f,:,:,:]\n",
        "                for h in range(H_new):       # traverse height\n",
        "                    for w in range(W_new):   # traverse width\n",
        "                        v0,v1 = h*stride, h*stride + H_k\n",
        "                        h0,h1 = w*stride, w*stride + W_k\n",
        "                        this_window = this_img[v0:v1,h0:h1,:]\n",
        "\n",
        "                        Z[i, h, w, f] = conv_single_step(this_window, this_filter, this_bias) \n",
        "\n",
        "        assert Z.shape == (n_tensors, H_new, W_new, n_filters)\n",
        "\n",
        "        self.output = Z\n",
        "        self.cache = X, filters, bias, stride, padding\n",
        "        \n",
        "        return self.output\n",
        "\n",
        "    def backward(self, upstream_g, learning_rate):\n",
        "        \"\"\"\n",
        "        upstream_g (dL/dZ) -- n_tensors x H_up x W_up x c_up\n",
        "        cache (values from previous layers) -- (X, W, B, s, p)               \n",
        "        \n",
        "        Output:\n",
        "        dX -- dL/dX, shape n_tensors x H_down x W_down x c_down\n",
        "        dF -- dL/dW, shape n_filters x k x k x k\n",
        "        dB -- dL/dB, shape n_filters x 1 x 1 x 1\n",
        "        \"\"\"\n",
        "        X, filters, bias, stride, padding = self.cache\n",
        "\n",
        "        n_tensors, H_down, W_down, c_down = X.shape\n",
        "        n_filters, H_k,    W_k,    c_down = filters.shape\n",
        "        n_tensors, H_up,   W_up,   c_up   = upstream_g.shape\n",
        "        \n",
        "        dX       = np.zeros_like(X)                           \n",
        "        dfilters = np.zeros_like(filters)\n",
        "        dbias    = np.zeros_like(bias)\n",
        "\n",
        "        X_padded  = np.pad(X,  ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        dX_padded = np.pad(dX, ((0,0), (padding,padding), (padding,padding), (0,0)), 'constant', constant_values = (0,0))\n",
        "        \n",
        "        for i in range(n_tensors):                       \n",
        "            x = X_padded[i]\n",
        "            dx = dX_padded[i]\n",
        "            \n",
        "            for h in range(H_up):                   # traverse height\n",
        "                for w in range(W_up):               # traverse width\n",
        "                    for c in range(c_up):           # traverse filters\n",
        "                        \n",
        "                        v0,v1 = h,h+H_k\n",
        "                        h0,h1 = w,w+W_k\n",
        "                        \n",
        "                        x_window = x[v0:v1, h0:h1, :]\n",
        "                        f_window = filters[c,:,:,:]\n",
        "\n",
        "                        dx_local = hard_tanh(f_window-x_window)\n",
        "                        df_local = x_window-f_window\n",
        "\n",
        "                        g = upstream_g[i, h, w, c]\n",
        "\n",
        "                        dx[v0:v1, v0:v1, :] += np.multiply(dx_local,g)\n",
        "                        dfilters[c,:,:,:]   += np.multiply(df_local,g)\n",
        "                        dbias[c,:,:,:]      += g\n",
        "                        \n",
        "            dX[i, :, :, :] = dx[padding:-padding, padding:-padding, :]\n",
        "        \n",
        "        assert(dX.shape == (n_tensors, H_down, W_down, c_down))\n",
        "\n",
        "        self.filters -= learning_rate*dfilters\n",
        "        self.bias    -= learning_rate*dbias\n",
        "\n",
        "        return dX"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0puXBDT4sFDz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}